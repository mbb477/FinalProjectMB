[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "",
    "text": "The goal of this modeling document is to explain the process of model fitting for prediction of diabetes using the Diabetes Health Indicators Dataset from 2015. This data set is a subset of data taken from the Behavioral Risk Factor Surveillance System (BRFSS) run by the Centers for Disease Control and Prevention (CDC). This data is obtained through a health-related telephone survey. The Diabetes Health Indicators Dataset contains 21 variables and 253,680 observations.\nFor modeling, 3 candidate models will be fit using logistic regression, classification tree and random forest. This will be done using the caret package. Various options are available for measuring performance of the model. Accuracy is the most common measure and the default in caret for classification models, but log loss will be used here for all models.\nLog loss takes into account the uncertainty of predictions by penalizing models more heavily for incorrect predictions. This means that the further the prediction probability is from the actual value, the higher the log loss is. So a model with a lower log loss is more desireable. Log loss is preferred for this data because it is imbalanced, with 86% of the observations being non-diabeticss. If accuracy is used, the models predictions might be more biased towards the more frequent class and as a result could have high prediction accuracy for the more frequent class, but fail to identify many minority class cases. By penalizing models for incorrect predictions, log loss allows a more balanced view of performance for the classes."
  },
  {
    "objectID": "Modeling.html#logistic-regression",
    "href": "Modeling.html#logistic-regression",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nSimply stated, logistic regression calculates the probability of an event occurring based on a data set and uses this information for classification of a new observation. The event is binary, so typcially refers to yes/no or success/failure. Logistic regression is a generalized linear model that works by linking the response to a function linear in parameters using the logit function, which is the log odds of the event occurring. Since the goal is to use the Diabetes Health Indicators Data to predict whether an individual has prediabetes/diabetes or not, logistic regression is appropriate to use.\nFor fitting logistic regression in caret, we need to specify the method as glm, although there are other options. Since our response is binary, the family is “binomial”. Since we want to use log loss, we need to specify this as the metric in train as well as in the trainControl for summaryFunction. When using log loss, classProbs needs to be set at TRUE. For the model fits presented here, trainControl will use cross validation for the method using 5 folds.\nThe fit for Model 1 with all 21 variables is shown below. The log loss for the fit is 0.317. This would not be considered high but how good it is would require some kind of baseline.\n\nlogRegFit_M1 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain1,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss))\nlogRegFit_M1\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142062, 142061, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3168771\n\n\nThe summary for Model 1 fit is below.\n\nsummary(logRegFit_M1)\n\n\nCall:\nNULL\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                -5.8075675  0.1061653 -54.703  &lt; 2e-16 ***\nHighBPYes                   0.7236103  0.0176443  41.011  &lt; 2e-16 ***\nHighCholYes                 0.5292785  0.0163079  32.455  &lt; 2e-16 ***\nCholCheckYes                1.2553484  0.0827403  15.172  &lt; 2e-16 ***\nBMI                         0.0576228  0.0010903  52.850  &lt; 2e-16 ***\nSmokerYes                  -0.0363594  0.0159091  -2.285 0.022287 *  \nStrokeYes                   0.1428743  0.0298347   4.789 1.68e-06 ***\nHeartDiseaseorAttackYes     0.2494557  0.0212568  11.735  &lt; 2e-16 ***\nPhysActivityYes            -0.0570992  0.0172378  -3.312 0.000925 ***\nFruitsOne_or_more_per_day  -0.0316766  0.0164129  -1.930 0.053609 .  \nVeggiesOne_or_more_per_day -0.0403626  0.0190000  -2.124 0.033641 *  \nHvyAlcoholConsumpYes       -0.7936119  0.0466087 -17.027  &lt; 2e-16 ***\nAnyHealthcareYes            0.0597127  0.0400401   1.491 0.135877    \nNoDocbcCostYes             -0.0039970  0.0275594  -0.145 0.884685    \nGenHlth.L                   1.6484010  0.0349706  47.137  &lt; 2e-16 ***\nGenHlth.Q                  -0.3692101  0.0264086 -13.981  &lt; 2e-16 ***\nGenHlth.C                  -0.0651716  0.0199671  -3.264 0.001099 ** \n`GenHlth^4`                 0.0144172  0.0148045   0.974 0.330139    \nMentHlth                   -0.0026657  0.0010153  -2.625 0.008653 ** \nPhysHlth                   -0.0034423  0.0009638  -3.572 0.000355 ***\nDiffWalkYes                 0.1361700  0.0202681   6.718 1.84e-11 ***\nSexMale                     0.2724640  0.0161617  16.859  &lt; 2e-16 ***\nAge.L                       2.3052029  0.0809772  28.467  &lt; 2e-16 ***\nAge.Q                      -0.7731608  0.0761771 -10.150  &lt; 2e-16 ***\nAge.C                      -0.2258799  0.0707002  -3.195 0.001399 ** \n`Age^4`                    -0.0505754  0.0674966  -0.749 0.453675    \n`Age^5`                    -0.1626783  0.0648937  -2.507 0.012181 *  \n`Age^6`                     0.0839011  0.0608445   1.379 0.167912    \n`Age^7`                     0.0010880  0.0557622   0.020 0.984433    \n`Age^8`                     0.0143805  0.0498648   0.288 0.773049    \n`Age^9`                    -0.0064350  0.0436131  -0.148 0.882701    \n`Age^10`                   -0.0904905  0.0379455  -2.385 0.017091 *  \n`Age^11`                    0.0624109  0.0333952   1.869 0.061642 .  \n`Age^12`                    0.0261874  0.0284193   0.921 0.356807    \nEducation.L                -0.1459432  0.1434671  -1.017 0.309030    \nEducation.Q                 0.0029214  0.1305425   0.022 0.982146    \nEducation.C                 0.0126293  0.0925402   0.136 0.891447    \n`Education^4`              -0.0534792  0.0540876  -0.989 0.322785    \n`Education^5`              -0.0164560  0.0305847  -0.538 0.590545    \nIncome.L                   -0.3919325  0.0289449 -13.541  &lt; 2e-16 ***\nIncome.Q                   -0.0754943  0.0248551  -3.037 0.002386 ** \nIncome.C                   -0.0036273  0.0237681  -0.153 0.878705    \n`Income^4`                 -0.0240004  0.0237376  -1.011 0.311982    \n`Income^5`                 -0.0097538  0.0233498  -0.418 0.676148    \n`Income^6`                 -0.0108508  0.0228148  -0.476 0.634357    \n`Income^7`                  0.0131927  0.0225212   0.586 0.558017    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 112441  on 177531  degrees of freedom\nAIC: 112533\n\nNumber of Fisher Scoring iterations: 6\n\n\nModel 2 is fit the same way. Even though variables were carefully selected using EDA, the log loss is slightly worse than that for Model 1, with a value of 0.319.\n\nlogRegFit_M2 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain2,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss))\nlogRegFit_M2\n\nGeneralized Linear Model \n\n177577 samples\n    13 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142062, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3186817\n\n\nThe summary of Model 2 is below.\n\nsummary(logRegFit_M2)\n\n\nCall:\nNULL\n\nCoefficients:\n                          Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)             -4.5391110  0.0423776 -107.111  &lt; 2e-16 ***\nHighBPYes                0.7533312  0.0176006   42.801  &lt; 2e-16 ***\nHighCholYes              0.5384371  0.0162638   33.106  &lt; 2e-16 ***\nBMI                      0.0577817  0.0010819   53.410  &lt; 2e-16 ***\nStrokeYes                0.1479984  0.0297927    4.968 6.78e-07 ***\nHeartDiseaseorAttackYes  0.2973285  0.0210240   14.142  &lt; 2e-16 ***\nPhysActivityYes         -0.0544293  0.0169485   -3.211 0.001321 ** \nHvyAlcoholConsumpYes    -0.7870973  0.0462350  -17.024  &lt; 2e-16 ***\nGenHlth.L                1.6873671  0.0347504   48.557  &lt; 2e-16 ***\nGenHlth.Q               -0.3614216  0.0263392  -13.722  &lt; 2e-16 ***\nGenHlth.C               -0.0650016  0.0199257   -3.262 0.001106 ** \n`GenHlth^4`              0.0146550  0.0147723    0.992 0.321168    \nMentHlth                -0.0039172  0.0010056   -3.895 9.81e-05 ***\nPhysHlth                -0.0036978  0.0009611   -3.847 0.000119 ***\nDiffWalkYes              0.1143075  0.0201263    5.680 1.35e-08 ***\nAge.L                    2.2848968  0.0803946   28.421  &lt; 2e-16 ***\nAge.Q                   -0.7524727  0.0760543   -9.894  &lt; 2e-16 ***\nAge.C                   -0.2595695  0.0706335   -3.675 0.000238 ***\n`Age^4`                 -0.0329026  0.0673990   -0.488 0.625425    \n`Age^5`                 -0.1720643  0.0648336   -2.654 0.007956 ** \n`Age^6`                  0.0837446  0.0607780    1.378 0.168241    \n`Age^7`                  0.0088728  0.0556861    0.159 0.873405    \n`Age^8`                  0.0157097  0.0497889    0.316 0.752363    \n`Age^9`                 -0.0111388  0.0435379   -0.256 0.798073    \n`Age^10`                -0.0903360  0.0378650   -2.386 0.017045 *  \n`Age^11`                 0.0612326  0.0333078    1.838 0.066006 .  \n`Age^12`                 0.0262475  0.0283345    0.926 0.354266    \nIncome.L                -0.3502377  0.0264531  -13.240  &lt; 2e-16 ***\nIncome.Q                -0.0569607  0.0243932   -2.335 0.019538 *  \nIncome.C                -0.0041762  0.0236997   -0.176 0.860127    \n`Income^4`              -0.0237810  0.0236771   -1.004 0.315192    \n`Income^5`              -0.0074909  0.0232958   -0.322 0.747790    \n`Income^6`              -0.0109228  0.0227628   -0.480 0.631334    \n`Income^7`               0.0103935  0.0224654    0.463 0.643621    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 113093  on 177543  degrees of freedom\nAIC: 113161\n\nNumber of Fisher Scoring iterations: 6\n\n\nFor Model 3 includes the fewest variables, and has the worst log loss, although it isn’t that different at 0.320. However\n\nlogRegFit_M3 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain3,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss))\nlogRegFit_M3\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142062, 142061, 142061 \nResampling results:\n\n  logLoss \n  0.319831\n\n\nThe summary of Model 3 is below.\n\nsummary(logRegFit_M3)\n\n\nCall:\nNULL\n\nCoefficients:\n                         Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)             -4.626176   0.041416 -111.701  &lt; 2e-16 ***\nHighBPYes                0.748884   0.017548   42.677  &lt; 2e-16 ***\nHighCholYes              0.535599   0.016214   33.033  &lt; 2e-16 ***\nBMI                      0.059374   0.001068   55.580  &lt; 2e-16 ***\nHeartDiseaseorAttackYes  0.322682   0.020738   15.560  &lt; 2e-16 ***\nPhysActivityYes         -0.055260   0.016737   -3.302 0.000961 ***\nGenHlth.L                1.668667   0.030921   53.966  &lt; 2e-16 ***\nGenHlth.Q               -0.378702   0.025334  -14.948  &lt; 2e-16 ***\nGenHlth.C               -0.072468   0.019863   -3.648 0.000264 ***\n`GenHlth^4`              0.014621   0.014726    0.993 0.320757    \nAge.L                    2.381773   0.079914   29.804  &lt; 2e-16 ***\nAge.Q                   -0.736736   0.076075   -9.684  &lt; 2e-16 ***\nAge.C                   -0.252703   0.070693   -3.575 0.000351 ***\n`Age^4`                 -0.030813   0.067445   -0.457 0.647775    \n`Age^5`                 -0.168704   0.064867   -2.601 0.009302 ** \n`Age^6`                  0.086306   0.060798    1.420 0.155737    \n`Age^7`                  0.014041   0.055708    0.252 0.801007    \n`Age^8`                  0.011945   0.049818    0.240 0.810499    \n`Age^9`                 -0.005493   0.043557   -0.126 0.899650    \n`Age^10`                -0.089525   0.037873   -2.364 0.018088 *  \n`Age^11`                 0.060742   0.033307    1.824 0.068197 .  \n`Age^12`                 0.027491   0.028322    0.971 0.331716    \nIncome.L                -0.377760   0.025878  -14.598  &lt; 2e-16 ***\nIncome.Q                -0.062488   0.024326   -2.569 0.010206 *  \nIncome.C                -0.001657   0.023639   -0.070 0.944109    \n`Income^4`              -0.024261   0.023622   -1.027 0.304380    \n`Income^5`              -0.006451   0.023241   -0.278 0.781326    \n`Income^6`              -0.011844   0.022704   -0.522 0.601913    \n`Income^7`               0.008511   0.022409    0.380 0.704079    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 113529  on 177548  degrees of freedom\nAIC: 113587\n\nNumber of Fisher Scoring iterations: 6\n\n\nOverall, Model 1 with 21 variables performed the best on the training set."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Classification Tree",
    "text": "Classification Tree\nClassification trees, also referred to as decision trees, are another way to predict classification.\nFitting a classification tree is similar to logistic regression except the method is “rpart” and we can add a tuning parameter. The tuning parameter is called the complexity parameter and this parameter helps control the size of the tree. The parameter is a threshold for improvement in the tree at each split and if this threshold is not met the node is not pursued. The complexity parameter also applies to pruning of the tree.\nThe tree fit for Model 1 is shown before. The algorithm determined that a complexity parameter of 0.001 produced the optimal model with a log loss of 0.356.\n\ntreeFit_M1 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain1,\n                  method = \"rpart\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                   tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit_M1\n\nCART \n\n177577 samples\n    21 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142062, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4449775\n  0.001  0.3551252\n  0.002  0.3551917\n  0.003  0.3552626\n  0.004  0.3553258\n  0.005  0.3553258\n  0.006  0.3844857\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\nThe tree fit for Model 2 is shown below. The optimal model also had a log loss of 0.356, but in this case used a complexity parameter of 0.002.\n\ntreeFit_M2 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain2,\n                  method = \"rpart\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                   tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit_M2\n\nCART \n\n177577 samples\n    13 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142062, 142062, 142061 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3974042\n  0.001  0.3557157\n  0.002  0.3559032\n  0.003  0.3559032\n  0.004  0.3559556\n  0.005  0.3559854\n  0.006  0.3748089\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\nThe tree fit for Model 3 is shown below. The algorithm in this case selected a cp of 0, corresponding to a log loss of 0.353. A cp of 0 suggests that the full tree is used with no pruning.\n\ntreeFit_M3 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain3,\n                  method = \"rpart\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                   tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit_M3\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142062, 142062, 142061 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3513916\n  0.001  0.3556906\n  0.002  0.3557023\n  0.003  0.3558871\n  0.004  0.3558871\n  0.005  0.3558934\n  0.006  0.3846496\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\nOverall, Model 3 is the best model of the three for classification tree fitting."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Random Forest",
    "text": "Random Forest"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis of the Diabetes Health Indicators Set",
    "section": "",
    "text": "Introduction\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a system run by the Centers for Disease Control and Prevention (CDC) that uses telephone surveys to collect health-related information from individuals in the United States every year. The system began in 1984 and collects more than 400,000 responses each year. The Diabetes Health Indicators Dataset that will be discussed here is a subset of the data obtained from the BRFSS for 2015. This data set contains 21 of the original 330 variables and 253.680 of the 441,455 original responses. Retention of the 21 variables in this data was based on what was believed to have potential relevance for diabetes.\nThe goal of this document is to use exploratory data analysis (EDA) to understand the data through validation, examining missingness, cleaning up the data and investigating distributions. This information can also help us decide what variables to include in our model. The end goal of this project is to obtain the best model to predict diabetes. Details of the EDA are presented below. The response variable is Diabetes_binary, where 0 indicates that the individual does not have diabetes and 1 means the individual has prediabetes or diabetes.\nThe variables chosen to be included in the modeling process are HighBP, HighChol, BMI, Stroke, HeartDiseaseorAttack, PhysActivity, HvyAlcoholConsump, GenHlth, MentHlth, PhysHlth, DiffWalk, Age and Income. HighBP corresponds to whether an individual has ever been told they have high blood pressure. HighChol corresponds to whether an individual has ever been told that they have high cholesterol. Both of these variables are often present in diabetics and are part of the conditions encompassing metabolic syndrome. BMI is body mass index and is known to greatly increase diabetes risk as the increased weigh increases insulin resistance. Stroke corresponds to whether and individual has ever had a stroke or not. HeartDiseaseorAttack corresponds to whether and individual has ever had a heart attack or has heart disease. The risk of both of these conditions is increased in diabetics. PhysActivity corresponds to whether an individual has exercised in the last 30 days. It does not indicate anything about frequency over those 30 days. HvyAlcoholConsump corresponds to whether and individual consumes a lot of alcohol. “Heavy” consumption is referred to as greater than 14 drink a week for males and greater than 7 drinks a week for women. GenHlth is a variable includes 5 levels, used to categorized how an individual viewed their general health at the time of the survey. MentHlh and PhysHlth correspond to the number of days in the last 30 days the individual felt that their mental health or physical health were poor. DiffWalk is a yes/no response indicting whether the individual has difficulty walking or climbing stairs. Age indicates the age grouping of the individual. The 13 groupings start at age 18-24, with subsequent levels in increments of 5 years and the final level corresponding to 80+ years. Finally, income corresponds to 8 levels generally in 5K increments, ranging from less than 10K, to greater than 75k. The details of selecting the 13 variables described here are discussed below for the EDA.\n\n\nEDA Analysis\nThe first step is to read in the data. Since the data is a csv file, this can be done using read_csv() from tidyverse.\n\n#load libraries\nlibrary(tidyverse)\nlibrary(psych)\n# read in data set\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\",\n                          show_col_types = FALSE)\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nThe first data step in EDA is to check for missing values. This data was already cleaned so missing values are not expected. Using is.na() shows that there are in fact no missing values.\n\n# check for missing values in each column\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThe distinct() function can be used to verify that there are no missing values in a form other than NA. Looking at the output below, we can see that there is nothing unusual. Most of the categories have 0’s and 1’s which generally correspond to no (0) and yes(1), with the exception of Fruits, Veggies and Sex. The categories with other numbers have various meanings depending on the category but the numbers are consistent with what is expected based on the kaggle website’s Diabetes Health Indicators Dataset Notebook.\n\n#check for unusual values that could indicate missingness\nunique_values &lt;- diabetes_data |&gt;\n  distinct()\nunique_values\n\n# A tibble: 229,474 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 229,464 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nThe next step is to summarize the variables. The simplest method for doing so is to use describe() from the psych package. Below is the summary for this data. As we would expect, most variables have a minimum of 0 and a maximum of 1. BMI, MentHlth and PhysHlth are the only variables that are truly numeric and this is reflected in the summary. One thing that stands out is that the max value for BMI is 98, while the average is 28.28. While a BMI of 98 is theoretically possible, it would likely be rare. This should be examined further.\n\n# summarize variables\ndescribe(diabetes_data)\n\n                     vars      n  mean   sd median trimmed  mad min max range\nDiabetes_binary         1 253680  0.14 0.35      0    0.05 0.00   0   1     1\nHighBP                  2 253680  0.43 0.49      0    0.41 0.00   0   1     1\nHighChol                3 253680  0.42 0.49      0    0.41 0.00   0   1     1\nCholCheck               4 253680  0.96 0.19      1    1.00 0.00   0   1     1\nBMI                     5 253680 28.38 6.61     27   27.68 4.45  12  98    86\nSmoker                  6 253680  0.44 0.50      0    0.43 0.00   0   1     1\nStroke                  7 253680  0.04 0.20      0    0.00 0.00   0   1     1\nHeartDiseaseorAttack    8 253680  0.09 0.29      0    0.00 0.00   0   1     1\nPhysActivity            9 253680  0.76 0.43      1    0.82 0.00   0   1     1\nFruits                 10 253680  0.63 0.48      1    0.67 0.00   0   1     1\nVeggies                11 253680  0.81 0.39      1    0.89 0.00   0   1     1\nHvyAlcoholConsump      12 253680  0.06 0.23      0    0.00 0.00   0   1     1\nAnyHealthcare          13 253680  0.95 0.22      1    1.00 0.00   0   1     1\nNoDocbcCost            14 253680  0.08 0.28      0    0.00 0.00   0   1     1\nGenHlth                15 253680  2.51 1.07      2    2.45 1.48   1   5     4\nMentHlth               16 253680  3.18 7.41      0    1.04 0.00   0  30    30\nPhysHlth               17 253680  4.24 8.72      0    1.77 0.00   0  30    30\nDiffWalk               18 253680  0.17 0.37      0    0.09 0.00   0   1     1\nSex                    19 253680  0.44 0.50      0    0.43 0.00   0   1     1\nAge                    20 253680  8.03 3.05      8    8.17 2.97   1  13    12\nEducation              21 253680  5.05 0.99      5    5.15 1.48   1   6     5\nIncome                 22 253680  6.05 2.07      7    6.35 1.48   1   8     7\n                      skew kurtosis   se\nDiabetes_binary       2.08     2.34 0.00\nHighBP                0.29    -1.92 0.00\nHighChol              0.31    -1.91 0.00\nCholCheck            -4.88    21.83 0.00\nBMI                   2.12    11.00 0.01\nSmoker                0.23    -1.95 0.00\nStroke                4.66    19.69 0.00\nHeartDiseaseorAttack  2.78     5.72 0.00\nPhysActivity         -1.20    -0.57 0.00\nFruits               -0.56    -1.69 0.00\nVeggies              -1.59     0.54 0.00\nHvyAlcoholConsump     3.85    12.85 0.00\nAnyHealthcare        -4.18    15.48 0.00\nNoDocbcCost           3.00     6.97 0.00\nGenHlth               0.42    -0.38 0.00\nMentHlth              2.72     6.44 0.01\nPhysHlth              2.21     3.50 0.02\nDiffWalk              1.77     1.15 0.00\nSex                   0.24    -1.94 0.00\nAge                  -0.36    -0.58 0.01\nEducation            -0.78     0.04 0.00\nIncome               -0.89    -0.28 0.00\n\n\nTo examine the BMI category, we can look at the unique values in the category. By doing so we can see the range of values present.\n\nunique_values &lt;- unique(diabetes_data$BMI)\nunique_values\n\n [1] 40 25 28 27 24 30 34 26 33 21 23 22 38 32 37 31 29 20 35 45 39 19 47 18 36\n[26] 43 55 49 42 17 16 41 44 50 59 48 52 46 54 57 53 14 15 51 58 63 61 56 74 62\n[51] 64 66 73 85 60 67 65 70 82 79 92 68 72 88 96 13 81 71 75 12 77 69 76 87 89\n[76] 84 95 98 91 86 83 80 90 78\n\n\nThe distribution of these values can be visualized using a density plot, specifically zooming in on the higher range of values.\n\n#plot distribution of BMI between 50 and 100\nggplot(diabetes_data, aes(x = BMI)) +\n  geom_density(fill = \"darksalmon\") + \n  xlim(50, 100)\n\nWarning: Removed 251133 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nTo see if these values might potentially be outliers, z-scores can be calculated. Generally, anything outside of +/- 3 is usually considered an outlier. This identifies 2963 potential outliers, in a BMI range of 49-98. This accounts for 1.17% of the data.\n\n#compute z-scores\ndiabetes_z&lt;- diabetes_data |&gt;\n  mutate(z_score = (BMI - 28.38) / 6.61)\n#view and determine number of outliers\ndiabetes_z_outlier &lt;- diabetes_z |&gt;\n  filter(abs(z_score) &gt; 3)\ndiabetes_z_outlier\n\n# A tibble: 2,963 × 23\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               1      1        1         1    55      0      0\n 2               0      1        1         1    49      1      1\n 3               1      1        1         1    49      0      1\n 4               0      0        0         1    50      1      0\n 5               1      1        0         1    59      0      0\n 6               0      1        0         1    50      0      0\n 7               1      1        1         1    52      1      0\n 8               0      0        0         1    55      0      0\n 9               0      1        1         1    54      1      0\n10               0      0        0         0    49      1      0\n# ℹ 2,953 more rows\n# ℹ 16 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;,\n#   z_score &lt;dbl&gt;\n\n\nMorbid obesity is considered to be anything over 40. According to the National Health and Nutrition Examination Survey (NHANES), 9.2% of adults were considered morbidly obese in 2018. To see how this diabetes indicators set aligns with this, counts can be calculated. By the counts in this data, 4.5% of individuals are morbidly obese. Since this is around half of the actual population incidence, the values classified as potential outliers will be retained to keep the data set as close to a population representation as possible.\n\ndiabetes_data |&gt;\n  filter(BMI &gt;40) |&gt;\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 11479\n\n\nFor the most of the data, since it is categorical, we can look at contingency tables to get an idea of their relationship with diabetes status. For this purpose, the data is going to be manipulated first. The categorical variables will be converted to factors and the levels renamed to better communicate what the levels actually mean. For this purpose, factor() will be use as it allows renaming of levels and specifying if a factor is ordered, as some of the categories are.\n\n#convert categorical columns to factors\n#Remove BMI outliers\ndiabetes &lt;- diabetes_data |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1),\n                                  labels = c(\"Non_Diabetic\", \"Prediabetic_Diabetic\")),\n         Education = factor(Education, ordered = TRUE, levels = c(1:6),\n                            labels = c(\"No_School\", \"Primary_and_Middle\",\n                                       \"Some_High_School\", \n                                       \"Graduated_High_School\", \"Some_College\",\n                                       \"Graduated_College\")),\n         Age = factor(Age,ordered = TRUE, levels = c(1:13), \n                       labels = c(\"Age_18to24\", \"Age_25to29\", \n                                  \"Age_30to34\", \"Age_35to39\",\n                                  \"Age_40to44\", \"Age_45to49\", \n                                  \"Age_50to54\", \"Age_55to59\", \n                                  \"Age_60to64\", \"Age_65to69\", \n                                  \"Age_70to74\", \"Age_75to79\", \n                                  \"Age_80_or_above\")),\n         Income = factor(Income, ordered = TRUE, levels = c(1:8),\n                         labels = c(\"Less_than_10K\", \n                                    \"From_10K_to_under_15K\",\n                                    \"From_15K_to_under_20K\",\n                                    \"From_20K_to_under_25K\", \n                                    \"From_25K_to_under_35K\",\n                                    \"From_35K_to_under_50K\", \n                                    \"From_50K_to_under_75K\",\n                                    \"From_75k_or_more\")),\n         Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n         DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = \n                                c(\"No\", \"Yes\")),\n         AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), \n                                labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), \n                                    labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity, levels = c(0, 1), \n                               labels = c(\"No\", \"Yes\")),\n         HeartDiseaseorAttack = factor(HeartDiseaseorAttack, \n                                       levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits, levels = c(0, 1), labels = \n                           c(\"None\", \"One_or_more_per_day\")),\n         Veggies = factor(Veggies, levels = c(0, 1), \n                          labels = c(\"None\", \"One_or_more_per_day\")),\n         GenHlth = factor(GenHlth, levels = c(1:5), \n                          labels = c(\"Excellent\", \"Very_Good\", \"Good\", \n                                     \"Fair\", \"Poor\")))\ndiabetes\n\n# A tibble: 253,680 × 22\n   Diabetes_binary      HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;                &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 Non_Diabetic         Yes    Yes      Yes          40 Yes    No    \n 2 Non_Diabetic         No     No       No           25 Yes    No    \n 3 Non_Diabetic         Yes    Yes      Yes          28 No     No    \n 4 Non_Diabetic         Yes    No       Yes          27 No     No    \n 5 Non_Diabetic         Yes    Yes      Yes          24 No     No    \n 6 Non_Diabetic         Yes    Yes      Yes          25 Yes    No    \n 7 Non_Diabetic         Yes    No       Yes          30 Yes    No    \n 8 Non_Diabetic         Yes    Yes      Yes          25 Yes    No    \n 9 Prediabetic_Diabetic Yes    Yes      Yes          30 Yes    No    \n10 Non_Diabetic         No     No       Yes          24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;ord&gt;, Education &lt;ord&gt;, Income &lt;ord&gt;\n\n\nNow that the data is properly formatted, contingency tables can be generated. The distribuition of males and females appears to be relatively similar, with males appearing to have a slightly higher propensity to have prediabetes or diabetes compared to women. Because the difference amounts to less than a few percent, this variable will be excluded from the modeling process.\n\n#Contingency table for sex\ndiabetes |&gt;\n  group_by(Sex, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Sex    Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 Female       123563                18411\n2 Male          94771                16935\n\n\nWe can also use bar charts to visualize the proportions of category levels relative to the diabetes variable. This visualization makes it easier to see the similarities or differences in proportions.\n\nggplot(diabetes, aes(x = Sex, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and \n       without Diabetes by Gender\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nIn looking at income, the prevalence of diabetes generally decreases as income increases, indicating that socioeconomic status may be an important predictor of diabetes. This variable will be included in the model fitting process.\n\n#Contingency table for income\ndiabetes |&gt;\n  group_by(Income, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 8 × 3\n  Income                Non_Diabetic Prediabetic_Diabetic\n  &lt;ord&gt;                        &lt;int&gt;                &lt;int&gt;\n1 Less_than_10K                 7428                 2383\n2 From_10K_to_under_15K         8697                 3086\n3 From_15K_to_under_20K        12426                 3568\n4 From_20K_to_under_25K        16081                 4054\n5 From_25K_to_under_35K        21379                 4504\n6 From_35K_to_under_50K        31179                 5291\n7 From_50K_to_under_75K        37954                 5265\n8 From_75k_or_more             83190                 7195\n\n\nThe bar chart for better visualization of Income is shown below.\n\nggplot(diabetes, aes(x = Income, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and \n       without Diabetes by Income Level\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext, we will examine the contingency table for eductation. Based on the output, it appears that the decreases with income. This is consistent with the contingency table for income, since income is generally associated with level of education. Looking at the bar chart, the patterns for income and education look nearly identical.\n\n#Contingency table for Education\ndiabetes |&gt;\n  group_by(Education, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 6 × 3\n  Education             Non_Diabetic Prediabetic_Diabetic\n  &lt;ord&gt;                        &lt;int&gt;                &lt;int&gt;\n1 No_School                      127                   47\n2 Primary_and_Middle            2860                 1183\n3 Some_High_School              7182                 2296\n4 Graduated_High_School        51684                11066\n5 Some_College                 59556                10354\n6 Graduated_College            96925                10400\n\n\nLooking at the bar chart, the patterns for income and education look nearly identical.\n\nggplot(diabetes, aes(x = Education, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and \n       without Diabetes by Education Level\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThe potential relationship between education and income should be examined because these two variables could be correlated and therefore redundant, which could affect the model predictions. To get a view of the relationship, a contingency table can be created. In looking at the output below, it appears that the number of individuals generally decreases as income level increases at the lower education levels, and increases at the higher education levels, suggesting that these variables are correlated.\n\n#contingency table for education and income\ncont_table &lt;- diabetes |&gt;\n  group_by(Education, Income) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Income, values_from = count)\ncont_table\n\n# A tibble: 6 × 9\n  Education            Less_than_10K From_10K_to_under_15K From_15K_to_under_20K\n  &lt;ord&gt;                        &lt;int&gt;                 &lt;int&gt;                 &lt;int&gt;\n1 No_School                       37                    25                    28\n2 Primary_and_Middle             900                   741                   740\n3 Some_High_School              1536                  1465                  1709\n4 Graduated_High_Scho…          3594                  4692                  6511\n5 Some_College                  2437                  3315                  4664\n6 Graduated_College             1307                  1545                  2342\n# ℹ 5 more variables: From_20K_to_under_25K &lt;int&gt;, From_25K_to_under_35K &lt;int&gt;,\n#   From_35K_to_under_50K &lt;int&gt;, From_50K_to_under_75K &lt;int&gt;,\n#   From_75k_or_more &lt;int&gt;\n\n\nWe can look into these variables further by testing independence with a Chi-Sqare test. The analysis, shown below, indicates that these variables are correlated, as indicated by the low p-value. Since education has a low frequency of numbers in the “No School” category, dropping this variable from the model fitting is a practical choice.\n\n#make a matrix and remove the Education column names\ncont_matrix &lt;- as.matrix(cont_table[, -1])\n#Chi-Square test\nchisquare_test &lt;- chisq.test(cont_matrix)\nchisquare_test\n\n\n    Pearson's Chi-squared test\n\ndata:  cont_matrix\nX-squared = 60337, df = 35, p-value &lt; 2.2e-16\n\n\nNext, we will examine the contingency table for age. Based on the output, it appears that the incidence of diabetes generally increases with age, so this variable will be included in the model fitting.\n\n#Contingency table for age\ndiabetes |&gt;\n  group_by(Age, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 13 × 3\n   Age             Non_Diabetic Prediabetic_Diabetic\n   &lt;ord&gt;                  &lt;int&gt;                &lt;int&gt;\n 1 Age_18to24              5622                   78\n 2 Age_25to29              7458                  140\n 3 Age_30to34             10809                  314\n 4 Age_35to39             13197                  626\n 5 Age_40to44             15106                 1051\n 6 Age_45to49             18077                 1742\n 7 Age_50to54             23226                 3088\n 8 Age_55to59             26569                 4263\n 9 Age_60to64             27511                 5733\n10 Age_65to69             25636                 6558\n11 Age_70to74             18392                 5141\n12 Age_75to79             12577                 3403\n13 Age_80_or_above        14154                 3209\n\n\nA visualization of the Age variable is presented below.\n\nggplot(diabetes, aes(x = Age, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of individuals with \n       and without diabetes by Age\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext, the relationship between the diabetes variable and Fruits will be examined. The proportions for no fruit and 1 or more a day are similar, 0.129 vs 0.158) so this variable will be excluded from modeling.\n\n#Contingency table for Fruits\ndiabetes |&gt;\n  group_by(Fruits, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Fruits              Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                      &lt;int&gt;                &lt;int&gt;\n1 None                       78129                14653\n2 One_or_more_per_day       140205                20693\n\n\nSimilarly, vegetable consumption does not appear to be an important variable for diabetes classification, with proportions of 0.18 and 0.13 for the diabetes category.\n\n#Contingency table for Veggies\ndiabetes |&gt;\n  group_by(Veggies, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Veggies             Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                      &lt;int&gt;                &lt;int&gt;\n1 None                       39229                 8610\n2 One_or_more_per_day       179105                26736\n\n\nThe contingency table for PhysActivity shows that the proportion of those with diabetes who do not exercise is nearly double that of those without diabetes who do exercise. This variable may be important for diabetes prediction, although it’s relevance may be more closely tied to how much a person exercises in the time period specified. The question to respondents was if they had engaged in any physical activity in the last 30 days so it does not take into account frequency of exercise in that time period. However, may people who exercise at all try to do so regularly, so this variable will be included in the modeling process.\n\n#Contingency table for PhysActivity\ndiabetes |&gt;\n  group_by(PhysActivity, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  PhysActivity Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;               &lt;int&gt;                &lt;int&gt;\n1 No                  48701                13059\n2 Yes                169633                22287\n\n\n\nggplot(diabetes, aes(x = PhysActivity, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of individuals with and \n       without diabetes by Physical Activity\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nThe HvyAlcoholConsump variable may be important for diabetes prediction. The contingency table below shows that the proportion of individuals who don’t drink alcohol heavily and have diabetes is close to 3 times that of those who do drink heavily and have diabetes.\n\n#Contingency table for HvyAlcoholConsum\ndiabetes |&gt;\n  group_by(HvyAlcoholConsump, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HvyAlcoholConsump Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                    &lt;int&gt;                &lt;int&gt;\n1 No                      204910                34514\n2 Yes                      13424                  832\n\n\n\nggplot(diabetes, aes(x = HvyAlcoholConsump, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of individuals with and without \n       diabetes by Alcohol Consumption Activity\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nThe NoDocbcCost represents whether an individual did or did not see a doctor because of the healthcare cost. The contingency table shows that the proportions of individuals with diabetes for the two NoDocBcCost levels were similar, at 0.136 and 0.175. Because of the similarity, this variable will not be included in model fitting.\n\n#Contingency table for NoDocbcCost\ndiabetes |&gt;\n  group_by(NoDocbcCost, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  NoDocbcCost Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;              &lt;int&gt;                &lt;int&gt;\n1 No                200722                31604\n2 Yes                17612                 3742\n\n\nThe AnyHealthcare variable is similar in proportion for both levels for those with diabetes, with proportions of 0.115 and 0.140. This variable will be excluded from model fitting.\n\n#Contingency table for AnyHealthcare\ndiabetes |&gt;\n  group_by(AnyHealthcare, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  AnyHealthcare Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                &lt;int&gt;                &lt;int&gt;\n1 No                   10995                 1422\n2 Yes                 207339                33924\n\n\nThe contingency table below for the HeartDiseaseorAttack variable suggests that this variable may be important for prediction. Nearly 32% of respondents who had heart disease or a heart attack had diabetes, compared to 12% of those who did not have these conditions.\n\n#Contingency table for NHeartDiseaseorAttack\ndiabetes |&gt;\n  group_by(HeartDiseaseorAttack, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HeartDiseaseorAttack Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                       &lt;int&gt;                &lt;int&gt;\n1 No                         202319                27468\n2 Yes                         16015                 7878\n\n\nBelow is a visualization of the HeartDiseaseorAttack variable.\n\nggplot(diabetes, aes(x = HeartDiseaseorAttack, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of individuals with and without \n       diabetes by Heart Disease or Heart Attack Status\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nThe contingency table for stroke is shown below. Nearly 32% of those respondents who had a stroke had diabetes, compared to those who didn’t have a stroke (13%). This suggests that this variable may be important for predicition.\n\n#Contingency table for Stroke\ndiabetes |&gt;\n  group_by(Stroke, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Stroke Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 No           211310                32078\n2 Yes            7024                 3268\n\n\nThe Smoker variable does not appear to be particularly important for diabetes prediction an the proportions are similar. This seems contradictory since it is known that smokers are more likely to have diabetes since nicotine makes cells less responsive to insulin. But the question for respondents was “Have you smoked at least 100 cigarettes (5 packs) in your entire life?” which communicates nothing about when in their life that was and if they are currently smoking. Consequently, this variable will be left out of the model fitting.\n\n#Contingency table for Smoker\ndiabetes |&gt;\n  group_by(Smoker, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Smoker Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 No           124228                17029\n2 Yes           94106                18317\n\n\n\nggplot(diabetes, aes(x = Smoker, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without \n       Diabetes by Smoking Status\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nThe contingency table for CholCheck shows that the proportions of those who have had their cholesterol checked in the last 5 years and those who have not and have diabetes are 0.025 and 0.144. While the difference may be significant, people who are sick or have chronic conditions are more likely to have their cholesterol checked more frequently than healthy individuals. Thus this variable is not necessarily important for prediction, but rather reflective of diabetes management, which may also be accompanied by other health conditions. For these reasons, this variable will be excluded from the model fitting.\n\n#Contingency table for CholCheck\ndiabetes |&gt;\n  group_by(CholCheck, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  CholCheck Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;            &lt;int&gt;                &lt;int&gt;\n1 No                9229                  241\n2 Yes             209105                35105\n\n\nThe contingency table for DiffWalk is below. For individuals with diabetes, the proportion of those who responded yes to difficulty walking or climbing stairs is 0.307 compared to that of those who responded no, 0.105. This variable may be important for predicting diabetes and will be included in model fitting.\n\n#Contingency table for DiffWalk\ndiabetes |&gt;\n  group_by(DiffWalk, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  DiffWalk Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;           &lt;int&gt;                &lt;int&gt;\n1 No             188780                22225\n2 Yes             29554                13121\n\n\nVisualization of DiffWalk relative to diabetes status is presented below.\n\nggplot(diabetes, aes(x = DiffWalk, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without \n       Diabetes by Difficulting Walking/Climbing Stairs\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nThe contingency table for HighChol shows proportions of 0.08 for those who don’t have high cholesterol but have diabetes, compared to 0.22 for those how have high cholesterol and diabetes. Since this is more than a 2-fold difference and diabetes and high blood pressure often occur together, this variable will be included in the model fitting.\n\n#Contingency table for HighChol\ndiabetes |&gt;\n  group_by(HighChol, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HighChol Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;           &lt;int&gt;                &lt;int&gt;\n1 No             134429                11660\n2 Yes             83905                23686\n\n\nA bar chart visualizing HighChol is below.\n\nggplot(diabetes, aes(x = HighChol, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without \n       Diabetes by Cholesterol Status\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nSimilar to HighChol, HighBP among those with diabetes was 4 times that of those without high blood pressure. High blood pressure often occurs in conjunction with diabetes as part of metabolic syndrome and so will be included in the model fitting.\n\n#Contingency table for HighBP\ndiabetes |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HighBP Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 No           136109                 8742\n2 Yes           82225                26604\n\n\nVisualization for HighBP is below and similar to that of HighChol.\n\nggplot(diabetes, aes(x = HighBP, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without \n       Diabetes by Blood Pressure Status\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\nWe can look at the distribution of BMI in this data by examining a histogram. The distribution is relatively normal with some right skew. Since the data is unbalanced, with more respondents not having diabetes, the skewed portion of the histogram likely corresponds to individuals with diabetes. Generally speaking, a BMI between 18.5-24.9 corresponds to a normal weight.\n\nggplot(data = diabetes, aes(BMI)) +\n  geom_histogram(binwidth = 2, fill = \"darksalmon\", color = \"black\") +\n  labs(y = \"Count\")\n\n\n\n\n\n\n\n\nTo view the strength of the relationship between BMI and diabetes, we can use point-biserial correlation, which allows calculation of the correlation between a dichotomous variable and a continuous variable. Point-biserial correlation is a modified form of Pearson correlation and can be performed using cor.test(). Both variables need to be numeric. The output p-value and “true correlation” indicate that the correlation between diabetes and BMI is significant, which is consistent with what is known about type II diabetes. This variable will be included in modeling.\n\n#convert Diabetes_binary to numeric\ndiabetes_corr &lt;- diabetes |&gt;\n  select(Diabetes_binary, BMI) |&gt;\n  mutate(Diabetes_binary = as.numeric(Diabetes_binary))\n#Perform Point-Biserial Correlation\ncor.test(diabetes_corr$BMI, diabetes_corr$Diabetes_binary)\n\n\n    Pearson's product-moment correlation\n\ndata:  diabetes_corr$BMI and diabetes_corr$Diabetes_binary\nt = 111.88, df = 253678, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2131315 0.2205484\nsample estimates:\n      cor \n0.2168431 \n\n\nThe MentHlth variable measures the number of days out of the last 30 the individual felt that their physical health was not good. The means look different but the standard deviations are high.\n\ndiabetes |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(across(MentHlth, .fns = list(\"mean\" = mean,\n                                                 \"sd\" = sd),\n                                                 .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 3\n  Diabetes_binary      mean_MentHlth sd_MentHlth\n  &lt;fct&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n1 Non_Diabetic                  2.98        7.11\n2 Prediabetic_Diabetic          4.46        8.95\n\n\nThe significance of the difference in means can be determined using a t-test. The t-test indicates that the differences are significant, so this variable will be included in the modeling.\n\nt_test_MentHlth &lt;- t.test(MentHlth ~ Diabetes_binary, data = diabetes)\nt_test_MentHlth\n\n\n    Welch Two Sample t-test\n\ndata:  MentHlth by Diabetes_binary\nt = -29.695, df = 42872, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Non_Diabetic and group Prediabetic_Diabetic is not equal to 0\n95 percent confidence interval:\n -1.581710 -1.385835\nsample estimates:\n        mean in group Non_Diabetic mean in group Prediabetic_Diabetic \n                          2.978034                           4.461806 \n\n\nBelow is the distribution of the MentHlth variable. it is not normal, but given the number of observation, the t-test holds due to the Central Limit Theorem.\n\nggplot(data = diabetes, aes(MentHlth)) +\n  geom_histogram(binwidth = 2, fill = \"darksalmon\", color = \"black\") +\n  labs(y = \"Count\")\n\n\n\n\n\n\n\n\nThe PhysHlth variable measures the number of days out of the last 30 the individual felt that their physical health was not good. The means look different but the standard deviations are high.\n\ndiabetes |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(across(PhysHlth, .fns = list(\"mean\" = mean,\n                                                 \"sd\" = sd),\n                                                 .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 3\n  Diabetes_binary      mean_PhysHlth sd_PhysHlth\n  &lt;fct&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n1 Non_Diabetic                  3.64        8.06\n2 Prediabetic_Diabetic          7.95       11.3 \n\n\nThe significance of the difference in means for PhysHlth can be determined using a t-test. The t-test indicates that the differences are significant, so this variable will be included in the modeling.\n\nt_test_PhysHlth &lt;- t.test(PhysHlth ~ Diabetes_binary, data = diabetes)\nt_test_PhysHlth\n\n\n    Welch Two Sample t-test\n\ndata:  PhysHlth by Diabetes_binary\nt = -68.969, df = 41367, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Non_Diabetic and group Prediabetic_Diabetic is not equal to 0\n95 percent confidence interval:\n -4.435979 -4.190814\nsample estimates:\n        mean in group Non_Diabetic mean in group Prediabetic_Diabetic \n                          3.641082                           7.954479 \n\n\nBelow is the distribution of the PhysHlth variable. it is not normal, but given the number of observation, the t-test holds.\n\nggplot(data = diabetes, aes(PhysHlth)) +\n  geom_histogram(binwidth = 2, fill = \"darksalmon\", color = \"black\") +\n  labs(y = \"Count\")\n\n\n\n\n\n\n\n\nThe contingency table of the GenHlth variable shows that a significantly higher proportion of non-diabetics viewed their health as being “excellent” compared to diabetics. For each subsequent level of progressively worsening opinion of general health, the proportion of diabetics increased while that of non-diabetics decreased. This variable may be important for prediction and will be included in the model fitting.\n\n#Contingency table for GenHlth\ndiabetes |&gt;\n  group_by(GenHlth, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 5 × 3\n  GenHlth   Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;            &lt;int&gt;                &lt;int&gt;\n1 Excellent        44159                 1140\n2 Very_Good        82703                 6381\n3 Good             62189                13457\n4 Fair             21780                 9790\n5 Poor              7503                 4578\n\n\nA visualization of the general health variable is shown below.\n\nggplot(diabetes, aes(x = GenHlth, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of individuals with and without \n       Diabetes by General Health Status\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\n\nData export for Modeling\nA csv file is being created to import into the Modeling document. Unfortunately the categorical variables will need to be converted to factors again, but the labels will not need to be created again.\n\nwrite_csv(diabetes, \"diabetes.csv\")\n\n\n\n\nConclusion\nBased on the exploratory data analysis, The variables have been reduced down to 13 from 21 and will be subsequently used in the model fitting. The model fitting can be accessed in the link below.\nModel Fitting"
  }
]