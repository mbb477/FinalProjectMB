[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "",
    "text": "The goal of this document is to explain the process and results of model fitting for prediction of diabetes using the Diabetes Health Indicators Dataset from 2015. This data set is a subset of data taken from the Behavioral Risk Factor Surveillance System (BRFSS) run by the Centers for Disease Control and Prevention (CDC). This data is obtained through a health-related telephone survey that is conducted every year. The Diabetes Health Indicators Dataset contains 22 variables and 253,680 observations.\nFor modeling, 3 candidate models will be fit using logistic regression, classification tree and random forest. This will be done using the caret package. Various options are available for measuring performance of the model. Accuracy is the most common measure used and the default in caret for classification models, but log loss will be used here for all models due to the class imbalance in this data set.\nLog loss takes into account the uncertainty of predictions by penalizing models more heavily for incorrect predictions, particularly incorrect confident predictions. This means that the further the prediction probability is from the actual value, the higher the log loss is. So a model with a lower log loss is more desirable. Log loss is preferred for this data because it is imbalanced, with 86% of the observations being non-diabetics. If accuracy is used, the models predictions might be more biased towards the more frequent class and as a result could have high prediction accuracy for the more frequent class, but fail to identify many minority class cases. By penalizing models for incorrect predictions, log loss allows a more balanced view of performance for the classes."
  },
  {
    "objectID": "Modeling.html#logistic-regression",
    "href": "Modeling.html#logistic-regression",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe three models will first be fit using logistic regression. Simply stated, logistic regression calculates the probability of an event occurring based on a data set and uses this information for classification. The event is binary, so refers to yes/no or success/failure. Logistic regression is a generalized linear model that works by linking the response to a function linear in parameters using the logit function, which is the log odds of the event occurring. Since the goal is to use the Diabetes Health Indicators Dataset to predict whether an individual has prediabetes/diabetes or not, logistic regression is appropriate to use.\nFor fitting logistic regression in caret, we need to specify the method as glm, although there are other options available. Since our response is binary, the family is “binomial”. Since we want to use log loss, we need to specify this as the metric in train() as well as in the trainControl() using summaryFunction (mnLogLoss). Since log loss uses predicted probabilities to evaluate model performance, classProbs needs to be set at TRUE. For the model fits presented here, trainControl will use cross validation as the method (“cv”) and 5 folds (number).\nThe fit for model 1 with all 21 variables is shown below. The log loss for the fit is 0.317.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run logistic regression on Model 1\nlogRegFit_M1 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain1,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss))\nlogRegFit_M1\n\nGeneralized Linear Model \n\n177577 samples\n    21 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3168564\n\n\nThe summary for model 1 fit is below.\n\nsummary(logRegFit_M1)\n\n\nCall:\nNULL\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                -5.8075675  0.1061653 -54.703  &lt; 2e-16 ***\nHighBPYes                   0.7236103  0.0176443  41.011  &lt; 2e-16 ***\nHighCholYes                 0.5292785  0.0163079  32.455  &lt; 2e-16 ***\nCholCheckYes                1.2553484  0.0827403  15.172  &lt; 2e-16 ***\nBMI                         0.0576228  0.0010903  52.850  &lt; 2e-16 ***\nSmokerYes                  -0.0363594  0.0159091  -2.285 0.022287 *  \nStrokeYes                   0.1428743  0.0298347   4.789 1.68e-06 ***\nHeartDiseaseorAttackYes     0.2494557  0.0212568  11.735  &lt; 2e-16 ***\nPhysActivityYes            -0.0570992  0.0172378  -3.312 0.000925 ***\nFruitsOne_or_more_per_day  -0.0316766  0.0164129  -1.930 0.053609 .  \nVeggiesOne_or_more_per_day -0.0403626  0.0190000  -2.124 0.033641 *  \nHvyAlcoholConsumpYes       -0.7936119  0.0466087 -17.027  &lt; 2e-16 ***\nAnyHealthcareYes            0.0597127  0.0400401   1.491 0.135877    \nNoDocbcCostYes             -0.0039970  0.0275594  -0.145 0.884685    \nGenHlth.L                   1.6484010  0.0349706  47.137  &lt; 2e-16 ***\nGenHlth.Q                  -0.3692101  0.0264086 -13.981  &lt; 2e-16 ***\nGenHlth.C                  -0.0651716  0.0199671  -3.264 0.001099 ** \n`GenHlth^4`                 0.0144172  0.0148045   0.974 0.330139    \nMentHlth                   -0.0026657  0.0010153  -2.625 0.008653 ** \nPhysHlth                   -0.0034423  0.0009638  -3.572 0.000355 ***\nDiffWalkYes                 0.1361700  0.0202681   6.718 1.84e-11 ***\nSexMale                     0.2724640  0.0161617  16.859  &lt; 2e-16 ***\nAge.L                       2.3052029  0.0809772  28.467  &lt; 2e-16 ***\nAge.Q                      -0.7731608  0.0761771 -10.150  &lt; 2e-16 ***\nAge.C                      -0.2258799  0.0707002  -3.195 0.001399 ** \n`Age^4`                    -0.0505754  0.0674966  -0.749 0.453675    \n`Age^5`                    -0.1626783  0.0648937  -2.507 0.012181 *  \n`Age^6`                     0.0839011  0.0608445   1.379 0.167912    \n`Age^7`                     0.0010880  0.0557622   0.020 0.984433    \n`Age^8`                     0.0143805  0.0498648   0.288 0.773049    \n`Age^9`                    -0.0064350  0.0436131  -0.148 0.882701    \n`Age^10`                   -0.0904905  0.0379455  -2.385 0.017091 *  \n`Age^11`                    0.0624109  0.0333952   1.869 0.061642 .  \n`Age^12`                    0.0261874  0.0284193   0.921 0.356807    \nEducation.L                -0.1459432  0.1434671  -1.017 0.309030    \nEducation.Q                 0.0029214  0.1305425   0.022 0.982146    \nEducation.C                 0.0126293  0.0925402   0.136 0.891447    \n`Education^4`              -0.0534792  0.0540876  -0.989 0.322785    \n`Education^5`              -0.0164560  0.0305847  -0.538 0.590545    \nIncome.L                   -0.3919325  0.0289449 -13.541  &lt; 2e-16 ***\nIncome.Q                   -0.0754943  0.0248551  -3.037 0.002386 ** \nIncome.C                   -0.0036273  0.0237681  -0.153 0.878705    \n`Income^4`                 -0.0240004  0.0237376  -1.011 0.311982    \n`Income^5`                 -0.0097538  0.0233498  -0.418 0.676148    \n`Income^6`                 -0.0108508  0.0228148  -0.476 0.634357    \n`Income^7`                  0.0131927  0.0225212   0.586 0.558017    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 112441  on 177531  degrees of freedom\nAIC: 112533\n\nNumber of Fisher Scoring iterations: 6\n\n\nModel 2 is fit the same way. Even though variables were selected using EDA, the log loss is slightly higher than that for model 1, with a value of 0.319.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run logistic regression on Model 2\nlogRegFit_M2 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain2,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss))\nlogRegFit_M2\n\nGeneralized Linear Model \n\n177577 samples\n    13 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3186222\n\n\nThe summary of model 2 is below.\n\nsummary(logRegFit_M2)\n\n\nCall:\nNULL\n\nCoefficients:\n                          Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)             -4.5391110  0.0423776 -107.111  &lt; 2e-16 ***\nHighBPYes                0.7533312  0.0176006   42.801  &lt; 2e-16 ***\nHighCholYes              0.5384371  0.0162638   33.106  &lt; 2e-16 ***\nBMI                      0.0577817  0.0010819   53.410  &lt; 2e-16 ***\nStrokeYes                0.1479984  0.0297927    4.968 6.78e-07 ***\nHeartDiseaseorAttackYes  0.2973285  0.0210240   14.142  &lt; 2e-16 ***\nPhysActivityYes         -0.0544293  0.0169485   -3.211 0.001321 ** \nHvyAlcoholConsumpYes    -0.7870973  0.0462350  -17.024  &lt; 2e-16 ***\nGenHlth.L                1.6873671  0.0347504   48.557  &lt; 2e-16 ***\nGenHlth.Q               -0.3614216  0.0263392  -13.722  &lt; 2e-16 ***\nGenHlth.C               -0.0650016  0.0199257   -3.262 0.001106 ** \n`GenHlth^4`              0.0146550  0.0147723    0.992 0.321168    \nMentHlth                -0.0039172  0.0010056   -3.895 9.81e-05 ***\nPhysHlth                -0.0036978  0.0009611   -3.847 0.000119 ***\nDiffWalkYes              0.1143075  0.0201263    5.680 1.35e-08 ***\nAge.L                    2.2848968  0.0803946   28.421  &lt; 2e-16 ***\nAge.Q                   -0.7524727  0.0760543   -9.894  &lt; 2e-16 ***\nAge.C                   -0.2595695  0.0706335   -3.675 0.000238 ***\n`Age^4`                 -0.0329026  0.0673990   -0.488 0.625425    \n`Age^5`                 -0.1720643  0.0648336   -2.654 0.007956 ** \n`Age^6`                  0.0837446  0.0607780    1.378 0.168241    \n`Age^7`                  0.0088728  0.0556861    0.159 0.873405    \n`Age^8`                  0.0157097  0.0497889    0.316 0.752363    \n`Age^9`                 -0.0111388  0.0435379   -0.256 0.798073    \n`Age^10`                -0.0903360  0.0378650   -2.386 0.017045 *  \n`Age^11`                 0.0612326  0.0333078    1.838 0.066006 .  \n`Age^12`                 0.0262475  0.0283345    0.926 0.354266    \nIncome.L                -0.3502377  0.0264531  -13.240  &lt; 2e-16 ***\nIncome.Q                -0.0569607  0.0243932   -2.335 0.019538 *  \nIncome.C                -0.0041762  0.0236997   -0.176 0.860127    \n`Income^4`              -0.0237810  0.0236771   -1.004 0.315192    \n`Income^5`              -0.0074909  0.0232958   -0.322 0.747790    \n`Income^6`              -0.0109228  0.0227628   -0.480 0.631334    \n`Income^7`               0.0103935  0.0224654    0.463 0.643621    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 113093  on 177543  degrees of freedom\nAIC: 113161\n\nNumber of Fisher Scoring iterations: 6\n\n\nModel 3 includes the fewest variables and has the highest log loss, although it isn’t that different at 0.320. More important than log loss on the trained model is how the model generalizes to new data.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run logistic regression on Model 3\nlogRegFit_M3 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain3,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss))\nlogRegFit_M3\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3198399\n\n\nThe summary of model 3 is below.\n\nsummary(logRegFit_M3)\n\n\nCall:\nNULL\n\nCoefficients:\n                          Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)             -4.6571280  0.0392275 -118.721  &lt; 2e-16 ***\nHighBPYes                0.7495382  0.0175460   42.719  &lt; 2e-16 ***\nHighCholYes              0.5366971  0.0162155   33.098  &lt; 2e-16 ***\nBMI                      0.0598307  0.0010639   56.235  &lt; 2e-16 ***\nHeartDiseaseorAttackYes  0.3232129  0.0207417   15.583  &lt; 2e-16 ***\nGenHlth.L                1.7266609  0.0339903   50.799  &lt; 2e-16 ***\nGenHlth.Q               -0.3559608  0.0262311  -13.570  &lt; 2e-16 ***\nGenHlth.C               -0.0702951  0.0198837   -3.535 0.000407 ***\n`GenHlth^4`              0.0125401  0.0147408    0.851 0.394931    \nPhysHlth                -0.0026339  0.0009074   -2.903 0.003701 ** \nAge.L                    2.3956775  0.0799208   29.976  &lt; 2e-16 ***\nAge.Q                   -0.7436426  0.0761212   -9.769  &lt; 2e-16 ***\nAge.C                   -0.2502661  0.0707240   -3.539 0.000402 ***\n`Age^4`                 -0.0290299  0.0674754   -0.430 0.667029    \n`Age^5`                 -0.1686328  0.0648921   -2.599 0.009359 ** \n`Age^6`                  0.0851086  0.0608178    1.399 0.161692    \n`Age^7`                  0.0142138  0.0557264    0.255 0.798674    \n`Age^8`                  0.0114648  0.0498344    0.230 0.818047    \n`Age^9`                 -0.0044975  0.0435713   -0.103 0.917786    \n`Age^10`                -0.0899010  0.0378837   -2.373 0.017640 *  \n`Age^11`                 0.0610976  0.0333149    1.834 0.066663 .  \n`Age^12`                 0.0275538  0.0283247    0.973 0.330662    \nIncome.L                -0.3889062  0.0258814  -15.026  &lt; 2e-16 ***\nIncome.Q                -0.0646285  0.0243153   -2.658 0.007862 ** \nIncome.C                -0.0022834  0.0236391   -0.097 0.923048    \n`Income^4`              -0.0247702  0.0236208   -1.049 0.294336    \n`Income^5`              -0.0062340  0.0232401   -0.268 0.788512    \n`Income^6`              -0.0123309  0.0227036   -0.543 0.587044    \n`Income^7`               0.0086819  0.0224067    0.387 0.698408    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 113531  on 177548  degrees of freedom\nAIC: 113589\n\nNumber of Fisher Scoring iterations: 6\n\n\nOverall, model 1 with 21 variables is the best model based on the log loss."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Classification Tree",
    "text": "Classification Tree\nClassification trees, also referred to as decision trees, are another way to predict class membership. The predictor space is split into regions in using the Gini Index or Entropy. The Gini Index measures impurity of a node, which is an indication of how well a node separates the classes. After the tree is grown it is pruned back by removing nodes that do not provide much predictive improvement. For purposes of identifying class membership, classification trees assign membership based on the most prevalent class in the region. Because the diabetes data set has a categorical response variable, the classification tree is appropriate to use for prediction of the two classes.\nFitting a classification tree using caret is similar to logistic regression except the method is “rpart” and we can add a tuning parameter. The tuning parameter is called the complexity parameter (cp) and this parameter helps control the size of the tree. The parameter is used for the pruning process and the model evaluated for each cp using log loss. A lower cp means less pruning.\nThe tree fit for Model 1 is shown before. The algorithm determined that a complexity parameter of 0.002 produced the optimal model with a log loss of 0.356.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run tree fitting on Model 1\ntreeFit_M1 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain1,\n                  method = \"rpart\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                   tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit_M1\n\nCART \n\n177577 samples\n    21 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4405007\n  0.001  0.3559044\n  0.002  0.3558902\n  0.003  0.3559745\n  0.004  0.3559933\n  0.005  0.3656972\n  0.006  0.3748596\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.002.\n\n\nThe tree fit for Model 2 is shown below. The optimal model also had a log loss of 0.356 and used a complexity parameter of 0.002.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run tree fitting on Model 2\ntreeFit_M2 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain2,\n                  method = \"rpart\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                   tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit_M2\n\nCART \n\n177577 samples\n    13 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4058828\n  0.001  0.3559044\n  0.002  0.3558902\n  0.003  0.3559745\n  0.004  0.3559933\n  0.005  0.3656972\n  0.006  0.3748596\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.002.\n\n\nThe tree fit for Model 3 is shown below. As with the previous two models, the algorithm selected a cp of 0.002, corresponding to a log loss of 0.356.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run tree fitting on Model 3\ntreeFit_M3 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain3,\n                  method = \"rpart\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                   tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit_M3\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3684045\n  0.001  0.3559044\n  0.002  0.3558902\n  0.003  0.3559745\n  0.004  0.3559933\n  0.005  0.3656972\n  0.006  0.3748596\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.002.\n\n\nAll 3 models for classification tree fitting had the same log loss so all three will be used in the model comparisons to identify the best model overall."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Random Forest",
    "text": "Random Forest\nRandom forest is an ensemble learning method. Ensemble methods produce many trees and average across these trees to determine prediction, often resulting in better predictions. Random forests are similar to bagging, but instead of using all predictors, uses fewer, randomly selected predictors. This random selection of features occurs at each split. This helps reduce correlation and variance when there are particularly strong predictors present in the data. The process for random forests involves creating bootstrap samples to fit on which allows the creation of multiple trees to be averaged across. To create bootstrap samples, the data is treated as a population and resampled with replacement. The random forest method is applied to each of these samples, each time creating a tree. The algorithm for determining splits is the same as classification trees, which involves minimizing the Gini Index. The randomization of variables and use of bootstrap samples prevents the need for pruning for ensemble methods such as random forests. Although the word “average” has been used here, for classification this means using majority vote to determine group membership. Using random forest is of interest for the models proposed here because it allows assignment of observations to the two classes, Non_Diabetic or Prediabetic_Diabetic, but is often more advantageous than classification trees because it produces many trees and usually results in better predictions.\nRandom Forest fitting in the caret package uses “rf” for the method. Settings for trainControl are the same as before. Random forests have one available tuning parameter, mtry. The mtry tuning parameter refers to the number of randomly selected predictors to use. You can also specify the number of trees for the model to average, which also corresponds to the number of bootstrap samples. This parameter is ntree and the default in caret is 500 trees. A rule of thumb for mtry for classification trees is the square root of the number of parameters.\nModel 1 has 21 parameters so the mtry could be set to 5. The mtry can be set to a higher number, but can significantly increase computation time. Due to computational issues and poorer log loss compared to the previous models, ntree was set to 50 and mtry 1:21 to identify the best mtry for the model. The best mtry for ntree 50 was 16, with a log loss of 0.518. The model was run again with ntree 100 and mtry 13:16. In this case, mtry 15 was slightly better than mtry 16, with a log loss of 0.455 compared to 0.460.\nThe final fitting of Model 1 to a random forest is presented below, using ntree 200 and mtry 15. The log loss at this ntree is 0.421.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run random forest fitting on Model 1\nrfFit_M1 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain1,\n                  method = \"rf\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                  ntree = 200,\n                  tuneGrid = expand.grid(mtry = 15))\nrfFit_M1\n\nRandom Forest \n\n177577 samples\n    21 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.4211159\n\nTuning parameter 'mtry' was held constant at a value of 15\n\n\nFor Model 2, at ntree 50, log loss decreased as mtry increased, with a log loss of 2.77 at mtry 1 and a log loss of 0.811 at mtry 13. The model was run again using ntree 200 and mtry 13, resulting in a log loss of 0.65. Increasing ntree to 400 improved the log loss (0.599) but took too much time computationally. Since the mtry of 13 is the total number of predictors in this model, this random forest is actually equivalent to bagging.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run random forest fitting on Model 2\nrfFit_M2 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain2,\n                  method = \"rf\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                  ntree = 200,\n                  tuneGrid = expand.grid(mtry = 13))\nrfFit_M2\n\nRandom Forest \n\n177577 samples\n    13 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.6495957\n\nTuning parameter 'mtry' was held constant at a value of 13\n\n\nModel 3 was fit similarly to the other random forest models. A ntree of 50 was used to identify the best mtry and then that mtry was used with a ntree of 200. For this model, at ntree 50, the log loss decreased as mtry increased, with a mtry of 1 giving a log loss of 3.08 and mtry 8 giving a log loss of 1.22. The data was fit again at ntree 100 using mtry 7:8 and again mtry 8 gave the best log loss at 1.07. The data was then fit using ntree 200 and mtry 8, which is presented below. The log loss is 0.956. Since the optimal mtry is 8, the number of predictors in Model 3, this random forest is equivalent to bagging.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run random forest fitting on Model 3\nrfFit_M3 &lt;- train(Diabetes_binary ~ ., \n                  data = diabetesTrain3,\n                  method = \"rf\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", \n                                           number = 5,\n                                           classProbs = TRUE,\n                                           summaryFunction = mnLogLoss),\n                  ntree = 200,\n                  tuneGrid = data.frame(mtry = 8))\nrfFit_M3\n\nRandom Forest \n\n177577 samples\n     8 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss \n  0.955829\n\nTuning parameter 'mtry' was held constant at a value of 8\n\n\nIt is apparent and makes sense that log loss decreases as ntree increases and it would perhaps drop even more if higher ntree were used, but there is a point of diminishing returns and overall the random forest models do not appear to be as good as the logistic regression and classification tree models based off the log loss results.\nAnother option is to use the “ranger” method. It has additional tuning parameters and often computes faster than “rf”. Ranger can accept 3 tuning parameters, mtry, splitrule and min.node.size. The splitrule parameter is gini by default. Another option is extratrees, which introduces additional randomness and can be better for data that isn’t balanced. The min.node.size parameter refers to the minimum number of observations in terminal nodes. Higher node size makes the model simpler by requiring more observations to make a split.\nThe “ranger” method was only used on model 1 due to extensive computation times (hours) for optimizing tuning parameters. It was initially run with num.trees 50, mtry 1:21, splitrule extratrees and min.node.size of c(5, 10, 20, 30, 40, 50). The optimal model had an mtry of 5 and a min.node.size of 50. The log loss with these parameters and num.trees 200 is 0.321.\n\n#Set seed for reproducibility\nset.seed(100)\n#Run random forest on model 1\nrfFit_ranger_M1 &lt;- train(Diabetes_binary ~ .,\n                         data = diabetesTrain1,\n                         method = \"ranger\",\n                         metric = \"logLoss\",\n                         trControl = trainControl(method = \"cv\", \n                                       number = 5,\n                                       classProbs = TRUE,\n                                       summaryFunction = mnLogLoss),\n                         num.trees = 200,\n                         tuneGrid = expand.grid(mtry = 5, \n                                     splitrule = \"extratrees\",\n                                     min.node.size = 50))\n\nGrowing trees.. Progress: 95%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 96%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 96%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 95%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 96%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 68%. Estimated remaining time: 14 seconds.\n\nrfFit_ranger_M1\n\nRandom Forest \n\n177577 samples\n    21 predictor\n     2 classes: 'Non_Diabetic', 'Prediabetic_Diabetic' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142063, 142061, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3213257\n\nTuning parameter 'mtry' was held constant at a value of 5\nTuning\n parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 50\n\n\nOverall, the best fit was model 1 using “ranger”, giving a log loss of 0.322."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nNow that the model training is complete, the best models for each type of classification can be compared. The models to be compared are Model 1 logistic regression, Model 1 random forest and all 3 classification tree models since they all had the same log loss. The first step is to calculate predictions based on the model fit using the test data. This is done using predict(), where the fit is indicated along with newdata as the relevant test set. The type needs to be set to “prob” for probability since log loss was used in the model fitting. The predicted probabilities can be combined with the true class labels from the data (Diabetes_binary) to get an easy comparison of what the predicted class is compared to the true class.\nOnce the predicted probabilities are obtained, they can be used to calculate log loss for each model. This metric can be used to determine which model is the best at predicting. The probabilities can also be converted to class labels to produce a confusion matrix, which gives accuracy, but one must remember that if the model is poor at predicting the minority class and good at predicting the majority class, the accuracy can be misleading. In such cases, other metrics in the confusion matrix need to be considered to accurately access how the model performs.\n\nLogistic Regression Model 1\nA table of predicted values for the best logistic regression model is below. The actual classes from the test data were added using bind_cols() for purposes of comparison.\n\n#obtain predictions using test data\npred_logReg &lt;- predict(logRegFit_M1, newdata = diabetesTest1, type = \"prob\")\n#extract response column Diabetes_binary from test data\nActual &lt;- diabetesTest1$Diabetes_binary\n#combine predicted probabilities with actual class labels\npred_logReg_actual_table &lt;- bind_cols(pred_logReg, Actual = Actual)\npred_logReg_actual_table &lt;- as_tibble(pred_logReg_actual_table)\npred_logReg_actual_table\n\n# A tibble: 76,103 × 3\n   Non_Diabetic Prediabetic_Diabetic Actual              \n          &lt;dbl&gt;                &lt;dbl&gt; &lt;fct&gt;               \n 1        0.658              0.342   Non_Diabetic        \n 2        0.952              0.0482  Non_Diabetic        \n 3        0.944              0.0562  Non_Diabetic        \n 4        0.870              0.130   Non_Diabetic        \n 5        0.939              0.0613  Non_Diabetic        \n 6        0.998              0.00177 Non_Diabetic        \n 7        0.729              0.271   Non_Diabetic        \n 8        0.260              0.740   Prediabetic_Diabetic\n 9        0.418              0.582   Prediabetic_Diabetic\n10        0.910              0.0898  Non_Diabetic        \n# ℹ 76,093 more rows\n\n\nThe log Loss for this model on the test data can be obtained using logLoss() from the Metrics package. This function takes 2 arguments, both vectors, with one being the actual response values for Diabetes_binary in the test data, and the other the predictions, which are the probabilities for the positive class. To perform this calculation, though, the Diabetes_binary levels need to be numeric. After converting the Diabetes_binary column to numbers, log loss can be computed with vector arguments being Diabetes_binary from the test set and the positive class, Prediabetic_Diabetic, from the test set predictions. The log loss for this model is 0.318.\n\n#covert Diabetes_binary to numbers\nnumeric_diabetes1 &lt;- diabetesTest1 |&gt;\n  mutate(Diabetes_binary = \n           ifelse(Diabetes_binary == \"Prediabetic_Diabetic\", 1, 0))\n#calculate log loss for best logistic regression model\nlogReg_log_loss &lt;- logLoss(numeric_diabetes1$Diabetes_binary,\n                           pred_logReg$Prediabetic_Diabetic)\nlogReg_log_loss\n\n[1] 0.3179013\n\n\nTo obtain a confusion matrix, the probabilities need to be converted to class labels. This is done using predict() but specifying “raw” as the type instead of “prob”. The confusion matrix can be obtained using confusionMatrix(), which accepts the arguments of the predictions and the actual values from the Diabetes_binary column of the test set. The option for positive was specified to ensure the positive class was correctly assigned. In looking at the output, while the accuracy is 86.5%, it can be seen from the matrix that prediabetic/diabetics have close to the same number of false positives as true positives. This is reflected in the Positive Predictive Value metric, which indicates that when the model predicts prediabetics/diabetics, it is correct 55.1% of the time. This is in contrast to the non-diabetic class, which is predicted correctly 87.7% of the time. The relatively poor identification of the Prediabetic_Diabetic class is also reflected in the low sensitivity of 15.2%, which means that the model correctly identifies 15.2% of all actual prediabetic/diabetic cases.\n\n#obtain predictions based on class\npred_logReg_class &lt;- predict(logRegFit_M1, newdata = diabetesTest1, \n                              type = \"raw\")\n#calculate confusion matrix\nlogReg_confusion &lt;- confusionMatrix(pred_logReg_class, \n                                    diabetesTest1$Diabetes_binary,\n                                    positive = \"Prediabetic_Diabetic\")\nlogReg_confusion\n\nConfusion Matrix and Statistics\n\n                      Reference\nPrediction             Non_Diabetic Prediabetic_Diabetic\n  Non_Diabetic                64182                 8988\n  Prediabetic_Diabetic         1318                 1615\n                                              \n               Accuracy : 0.8646              \n                 95% CI : (0.8621, 0.867)     \n    No Information Rate : 0.8607              \n    P-Value [Acc &gt; NIR] : 0.0009204           \n                                              \n                  Kappa : 0.1897              \n                                              \n Mcnemar's Test P-Value : &lt; 2.2e-16           \n                                              \n            Sensitivity : 0.15232             \n            Specificity : 0.97988             \n         Pos Pred Value : 0.55063             \n         Neg Pred Value : 0.87716             \n             Prevalence : 0.13932             \n         Detection Rate : 0.02122             \n   Detection Prevalence : 0.03854             \n      Balanced Accuracy : 0.56610             \n                                              \n       'Positive' Class : Prediabetic_Diabetic\n                                              \n\n\n\n\nClassification Tree Model 1\nA table of predicted values for classification tree Model is below.\n\npred_tree1 &lt;- predict(treeFit_M1, newdata = diabetesTest1, type = \"prob\")\n#combine predicted probabilitie with actual class labels\npred_tree_actual_table1 &lt;- bind_cols(pred_tree1, Actual = Actual)\npred_tree_actual_table1 &lt;- as_tibble(pred_tree_actual_table1)\npred_tree_actual_table1\n\n# A tibble: 76,103 × 3\n   Non_Diabetic Prediabetic_Diabetic Actual              \n          &lt;dbl&gt;                &lt;dbl&gt; &lt;fct&gt;               \n 1        0.681               0.319  Non_Diabetic        \n 2        0.940               0.0602 Non_Diabetic        \n 3        0.940               0.0602 Non_Diabetic        \n 4        0.940               0.0602 Non_Diabetic        \n 5        0.813               0.187  Non_Diabetic        \n 6        0.940               0.0602 Non_Diabetic        \n 7        0.813               0.187  Non_Diabetic        \n 8        0.396               0.604  Prediabetic_Diabetic\n 9        0.681               0.319  Prediabetic_Diabetic\n10        0.813               0.187  Non_Diabetic        \n# ℹ 76,093 more rows\n\n\nThe log loss for Model 1 on the test data is below. The log loss for this model on the test set is 0.357.\n\n#calculate log loss for best random forest model\n#use numeric_diabetes1 from logistic regression since it is the same test set\ntree_log_loss1 &lt;- logLoss(numeric_diabetes1$Diabetes_binary,\n                           pred_tree1$Prediabetic_Diabetic)\ntree_log_loss1\n\n[1] 0.3569434\n\n\nThe confusion matrix was computed as with the logistic regression model. Accuracy is 86.5%, but is biased because it is better at predicting the Non_Diabetic class. For the Prediabetic_Diabetic class, of all positive predictions, the model classifies correctly 57.1% of the time. In comparison, the Non_Diabetic class is accurately assigned 87.3% of the time. The sensitivity is also low, indicating that the model correctly identifies 11.6% of all actual prediabetic/diabetic cases. The accuracy is higher than the no information rate, but the small difference between the two also suggests that the model is not effectively distinguishing between the classes given the imbalance in the data.\n\n#obtain predictions based on class\npred_tree_class1 &lt;- predict(treeFit_M1, newdata = diabetesTest1, \n                              type = \"raw\")\n#calculate confusion matrix\ntree_confusion1 &lt;- confusionMatrix(pred_tree_class1, \n                                    diabetesTest1$Diabetes_binary,\n                                    positive = \"Prediabetic_Diabetic\")\ntree_confusion1\n\nConfusion Matrix and Statistics\n\n                      Reference\nPrediction             Non_Diabetic Prediabetic_Diabetic\n  Non_Diabetic                64575                 9374\n  Prediabetic_Diabetic          925                 1229\n                                              \n               Accuracy : 0.8647              \n                 95% CI : (0.8622, 0.8671)    \n    No Information Rate : 0.8607              \n    P-Value [Acc &gt; NIR] : 0.0007147           \n                                              \n                  Kappa : 0.1528              \n                                              \n Mcnemar's Test P-Value : &lt; 2.2e-16           \n                                              \n            Sensitivity : 0.11591             \n            Specificity : 0.98588             \n         Pos Pred Value : 0.57057             \n         Neg Pred Value : 0.87324             \n             Prevalence : 0.13932             \n         Detection Rate : 0.01615             \n   Detection Prevalence : 0.02830             \n      Balanced Accuracy : 0.55089             \n                                              \n       'Positive' Class : Prediabetic_Diabetic\n                                              \n\n\n\n\nClassification Tree Model 2\nNow we can examine Model 2 for the classification tree. The table of predictions is presented below.\n\n#obtain predictions using test data\npred_tree2 &lt;- predict(treeFit_M2, newdata = diabetesTest2, type = \"prob\")\n#combine predicted probabilitie with actual class labels\npred_tree_actual_table2 &lt;- bind_cols(pred_tree2, Actual = Actual)\npred_tree_actual_table2 &lt;- as_tibble(pred_tree_actual_table2)\npred_tree_actual_table2\n\n# A tibble: 76,103 × 3\n   Non_Diabetic Prediabetic_Diabetic Actual              \n          &lt;dbl&gt;                &lt;dbl&gt; &lt;fct&gt;               \n 1        0.681               0.319  Non_Diabetic        \n 2        0.940               0.0602 Non_Diabetic        \n 3        0.940               0.0602 Non_Diabetic        \n 4        0.940               0.0602 Non_Diabetic        \n 5        0.813               0.187  Non_Diabetic        \n 6        0.940               0.0602 Non_Diabetic        \n 7        0.813               0.187  Non_Diabetic        \n 8        0.396               0.604  Prediabetic_Diabetic\n 9        0.681               0.319  Prediabetic_Diabetic\n10        0.813               0.187  Non_Diabetic        \n# ℹ 76,093 more rows\n\n\nThe log loss for this model on the test data is below. The log loss is the same as that for Model 1 with 21 variables, 0.357.\n\n#covert Diabetes_binary to numbers\nnumeric_diabetes2 &lt;- diabetesTest2 |&gt;\n  mutate(Diabetes_binary = \n           ifelse(Diabetes_binary == \"Prediabetic_Diabetic\", 1, 0))\n#calculate log loss for best classification tree model\ntree_log_loss2 &lt;- logLoss(numeric_diabetes2$Diabetes_binary,\n                           pred_tree2$Prediabetic_Diabetic)\ntree_log_loss2\n\n[1] 0.3569434\n\n\nThe confusion matrix output is below. The results appear to be identical to the output for Model 1.\n\n#obtain predictions based on class\npred_tree_class2 &lt;- predict(treeFit_M2, newdata = diabetesTest2, \n                            type = \"raw\")\n#calculate confusion matrix\ntree_confusion2 &lt;- confusionMatrix(pred_tree_class2, \n                                    diabetesTest2$Diabetes_binary,\n                                    positive = \"Prediabetic_Diabetic\")\ntree_confusion2\n\nConfusion Matrix and Statistics\n\n                      Reference\nPrediction             Non_Diabetic Prediabetic_Diabetic\n  Non_Diabetic                64575                 9374\n  Prediabetic_Diabetic          925                 1229\n                                              \n               Accuracy : 0.8647              \n                 95% CI : (0.8622, 0.8671)    \n    No Information Rate : 0.8607              \n    P-Value [Acc &gt; NIR] : 0.0007147           \n                                              \n                  Kappa : 0.1528              \n                                              \n Mcnemar's Test P-Value : &lt; 2.2e-16           \n                                              \n            Sensitivity : 0.11591             \n            Specificity : 0.98588             \n         Pos Pred Value : 0.57057             \n         Neg Pred Value : 0.87324             \n             Prevalence : 0.13932             \n         Detection Rate : 0.01615             \n   Detection Prevalence : 0.02830             \n      Balanced Accuracy : 0.55089             \n                                              \n       'Positive' Class : Prediabetic_Diabetic\n                                              \n\n\n\n\nClassification Tree Model 3\nA table of predicted values for classification tree Model 3 is below.\n\n#obtain predictions using test data\npred_tree3 &lt;- predict(treeFit_M3, newdata = diabetesTest3, type = \"prob\")\n#combine predicted probabilities with actual class labels\npred_tree_actual_table3 &lt;- bind_cols(pred_tree3, Actual = Actual)\npred_tree_actual_table3 &lt;- as_tibble(pred_tree_actual_table3)\npred_tree_actual_table3\n\n# A tibble: 76,103 × 3\n   Non_Diabetic Prediabetic_Diabetic Actual              \n          &lt;dbl&gt;                &lt;dbl&gt; &lt;fct&gt;               \n 1        0.681               0.319  Non_Diabetic        \n 2        0.940               0.0602 Non_Diabetic        \n 3        0.940               0.0602 Non_Diabetic        \n 4        0.940               0.0602 Non_Diabetic        \n 5        0.813               0.187  Non_Diabetic        \n 6        0.940               0.0602 Non_Diabetic        \n 7        0.813               0.187  Non_Diabetic        \n 8        0.396               0.604  Prediabetic_Diabetic\n 9        0.681               0.319  Prediabetic_Diabetic\n10        0.813               0.187  Non_Diabetic        \n# ℹ 76,093 more rows\n\n\nThe log loss output for this model on the test data is below. The log loss for this model is 0.357, the same as that for models 1 and 2.\n\n#covert Diabetes_binary to numbers\nnumeric_diabetes3 &lt;- diabetesTest3 |&gt;\n  mutate(Diabetes_binary = \n           ifelse(Diabetes_binary == \"Prediabetic_Diabetic\", 1, 0))\n#calculate log loss for best classification tree model\ntree_log_loss3 &lt;- logLoss(numeric_diabetes3$Diabetes_binary,\n                          pred_tree3$Prediabetic_Diabetic)\ntree_log_loss3\n\n[1] 0.3569434\n\n\nThe confusion matrix for classification tree Model 3 is presented below. The output results are the same as those for models 1 and 2.\n\n#obtain predictions based on class\npred_tree_class3 &lt;- predict(treeFit_M3, newdata = diabetesTest3, \n                            type = \"raw\")\n#calculate confusion matrix\ntree_confusion3 &lt;- confusionMatrix(pred_tree_class3, \n                                   diabetesTest3$Diabetes_binary,\n                                   positive = \"Prediabetic_Diabetic\")\ntree_confusion3\n\nConfusion Matrix and Statistics\n\n                      Reference\nPrediction             Non_Diabetic Prediabetic_Diabetic\n  Non_Diabetic                64575                 9374\n  Prediabetic_Diabetic          925                 1229\n                                              \n               Accuracy : 0.8647              \n                 95% CI : (0.8622, 0.8671)    \n    No Information Rate : 0.8607              \n    P-Value [Acc &gt; NIR] : 0.0007147           \n                                              \n                  Kappa : 0.1528              \n                                              \n Mcnemar's Test P-Value : &lt; 2.2e-16           \n                                              \n            Sensitivity : 0.11591             \n            Specificity : 0.98588             \n         Pos Pred Value : 0.57057             \n         Neg Pred Value : 0.87324             \n             Prevalence : 0.13932             \n         Detection Rate : 0.01615             \n   Detection Prevalence : 0.02830             \n      Balanced Accuracy : 0.55089             \n                                              \n       'Positive' Class : Prediabetic_Diabetic\n                                              \n\n\nThe 3 classification tree models produced the same output. If a choice was to be made among the the three models, the most parsimonious model, Model 3 with 8 predictor variables, could be chosen.\n\n\nRandom Forest Model 1\nRecall that the best random forest model was model 1 using “ranger”. A table of predicted values is presented below.\n\n#obtain predictions using test data\npred_ranger &lt;- predict(rfFit_ranger_M1, newdata = diabetesTest1, type = \"prob\")\n\n#combine predicted probabilitie with actual class labels\npred_ranger_actual_table &lt;- bind_cols(pred_ranger, Actual = Actual)\npred_ranger_actual_table &lt;- as_tibble(pred_ranger_actual_table)\npred_ranger_actual_table\n\n# A tibble: 76,103 × 3\n   Non_Diabetic Prediabetic_Diabetic Actual              \n          &lt;dbl&gt;                &lt;dbl&gt; &lt;fct&gt;               \n 1        0.584               0.416  Non_Diabetic        \n 2        0.962               0.0376 Non_Diabetic        \n 3        0.921               0.0789 Non_Diabetic        \n 4        0.785               0.215  Non_Diabetic        \n 5        0.936               0.0642 Non_Diabetic        \n 6        0.989               0.0106 Non_Diabetic        \n 7        0.709               0.291  Non_Diabetic        \n 8        0.356               0.644  Prediabetic_Diabetic\n 9        0.599               0.401  Prediabetic_Diabetic\n10        0.914               0.0857 Non_Diabetic        \n# ℹ 76,093 more rows\n\n\nThe log loss for this model is 0.322.\n\n#calculate log loss for best random forest model\n#use numeric_diabetes1 since it is the same test set used for logistic regression\nranger_log_loss &lt;- logLoss(numeric_diabetes1$Diabetes_binary,\n                       pred_ranger$Prediabetic_Diabetic)\nranger_log_loss\n\n[1] 0.321795\n\n\nThe confusion matrix is below. The positive predictive value appears more promising than previous models at 61.8%, but the sensitivity is lower, with only 5.9% of all prediabetic/diabetic cases being correctly identified.\n\n#obtain predictions based on class\npred_ranger_class &lt;- predict(rfFit_ranger_M1, newdata = diabetesTest1, \n                           type = \"raw\")\n#calculate confusion matrix\nranger_confusion &lt;- confusionMatrix(pred_ranger_class, \n                                  diabetesTest1$Diabetes_binary,\n                                  positive = \"Prediabetic_Diabetic\")\nranger_confusion\n\nConfusion Matrix and Statistics\n\n                      Reference\nPrediction             Non_Diabetic Prediabetic_Diabetic\n  Non_Diabetic                65116                 9981\n  Prediabetic_Diabetic          384                  622\n                                              \n               Accuracy : 0.8638              \n                 95% CI : (0.8613, 0.8662)    \n    No Information Rate : 0.8607              \n    P-Value [Acc &gt; NIR] : 0.006338            \n                                              \n                  Kappa : 0.0851              \n                                              \n Mcnemar's Test P-Value : &lt; 2.2e-16           \n                                              \n            Sensitivity : 0.058663            \n            Specificity : 0.994137            \n         Pos Pred Value : 0.618290            \n         Neg Pred Value : 0.867092            \n             Prevalence : 0.139324            \n         Detection Rate : 0.008173            \n   Detection Prevalence : 0.013219            \n      Balanced Accuracy : 0.526400            \n                                              \n       'Positive' Class : Prediabetic_Diabetic\n                                              \n\n\nSimply for comparison purposes, the best random forest model using the “rf” method will be presented here as well. A table of predicted values for model 1 using “rf” is below.\n\n#obtain predictions using test data\npred_rf &lt;- predict(rfFit_M1, newdata = diabetesTest1, type = \"prob\")\n\n#combine predicted probabilitie with actual class labels\npred_rf_actual_table &lt;- bind_cols(pred_rf, Actual = Actual)\npred_rf_actual_table &lt;- as_tibble(pred_rf_actual_table)\npred_rf_actual_table\n\n# A tibble: 76,103 × 3\n   Non_Diabetic Prediabetic_Diabetic Actual              \n          &lt;dbl&gt;                &lt;dbl&gt; &lt;fct&gt;               \n 1        0.63                 0.37  Non_Diabetic        \n 2        0.995                0.005 Non_Diabetic        \n 3        0.985                0.015 Non_Diabetic        \n 4        0.73                 0.27  Non_Diabetic        \n 5        0.96                 0.04  Non_Diabetic        \n 6        0.975                0.025 Non_Diabetic        \n 7        0.715                0.285 Non_Diabetic        \n 8        0.225                0.775 Prediabetic_Diabetic\n 9        0.48                 0.52  Prediabetic_Diabetic\n10        0.96                 0.04  Non_Diabetic        \n# ℹ 76,093 more rows\n\n\nThe log loss for this model on the test data is infinity, indicating that the model does not generalize well to unseen data. This usually happens when the model is overly confident in its predictions (probabilities are close to 0 or 1) and those predictions are actually incorrect.\n\n#calculate log loss for best random forest model\n#use numeric_diabetes1 since it is the same test set used for logistic regression\nrf_log_loss &lt;- logLoss(numeric_diabetes1$Diabetes_binary,\n                       pred_rf$Prediabetic_Diabetic)\nrf_log_loss\n\n[1] Inf\n\n\nThe confusion matrix output is below. The accuracy is slightly lower compared to the previous models at 85.96% and the predictions assigned to the Prediabetic_Diabetic class were correct 48.9% of the time, lower than the other models. Interestingly, the sensitivity is 19.1%, which is not good, but better than the other models.\n\n#Obtain predictions based on class\npred_rf_class &lt;- predict(rfFit_M1, newdata = diabetesTest1, \n                           type = \"raw\")\n#Calculate confusion matrix\nrf_confusion &lt;- confusionMatrix(pred_rf_class, \n                                  diabetesTest1$Diabetes_binary,\n                                  positive = \"Prediabetic_Diabetic\")\nrf_confusion\n\nConfusion Matrix and Statistics\n\n                      Reference\nPrediction             Non_Diabetic Prediabetic_Diabetic\n  Non_Diabetic                63380                 8572\n  Prediabetic_Diabetic         2120                 2031\n                                              \n               Accuracy : 0.8595              \n                 95% CI : (0.857, 0.862)      \n    No Information Rate : 0.8607              \n    P-Value [Acc &gt; NIR] : 0.8256              \n                                              \n                  Kappa : 0.2137              \n                                              \n Mcnemar's Test P-Value : &lt;2e-16              \n                                              \n            Sensitivity : 0.19155             \n            Specificity : 0.96763             \n         Pos Pred Value : 0.48928             \n         Neg Pred Value : 0.88087             \n             Prevalence : 0.13932             \n         Detection Rate : 0.02669             \n   Detection Prevalence : 0.05454             \n      Balanced Accuracy : 0.57959             \n                                              \n       'Positive' Class : Prediabetic_Diabetic"
  },
  {
    "objectID": "Modeling.html#conclusion",
    "href": "Modeling.html#conclusion",
    "title": "Modeling of the Diabetes Health Indicators Set",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, the logistic regression model using all 21 predictor variables is the best model using log loss as a metric, with a log loss of 0.318. The classification trees produced a higher log loss of 0.357. The random forest model resulted in infinity log loss using the “rf” method and a log loss of 0.322 for “ranger”. The log loss of infinity indicates that the model’s predictions are unreliable due to extreme probability estimates. Looking at confusion matrices provided more information about the ability of the models to accurately predict the prediabetic/diabetic minority class. When the models predicted non-diabetics, those predictions were correct around 87% of the time. In contrast, the prediabetic/diabetic class predictions were correct 49-62% of the time, with the percentage of actual prediabetic/diabetic cases correctly identified by the models in the range of only 6-20%. These results are not unexpected given that the data is imbalanced. There are options to help balance the classes, such as undersampling and oversampling, that could improve predictions. These options are available in caret in trainControl(). Weights can also be used, which are available in the randomForest package and can be used for some methods in caret as well."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis of the Diabetes Health Indicators Set",
    "section": "",
    "text": "Introduction\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a system run by the Centers for Disease Control and Prevention (CDC) that uses telephone surveys to collect health-related information from individuals in the United States every year. The system began in 1984 and collects more than 400,000 responses each year. The Diabetes Health Indicators Dataset that will be discussed here is a subset of the data obtained from the BRFSS for 2015. This data set contains 22 of the original 330 variables and 253,680 of the 441,455 original responses. Retention of the 21 feature variables in this data was based on what was believed to have potential relevance for diabetes.\nThe goal of this document is to use exploratory data analysis (EDA) to understand the data through validation, examining missingness, cleaning up the data and investigating distributions. This information can also aid in deciding what predictors to use for building classification models. The end goal of this project is to obtain the best model to predict whether and individual has prediabetes/diabetes or not. Details of the EDA are presented below. The response variable is Diabetes_binary, where 0 indicates that the individual does not have diabetes and 1 means the individual has prediabetes or diabetes.\nThe variables chosen to be included in the modeling process are HighBP, HighChol, BMI, Stroke, HeartDiseaseorAttack, PhysActivity, HvyAlcoholConsump, GenHlth, MentHlth, PhysHlth, DiffWalk, Age and Income. HighBP corresponds to whether an individual has ever been told they have high blood pressure. HighChol corresponds to whether an individual has ever been told that they have high cholesterol. Both of these variables are often present in diabetics and are part of the conditions encompassing metabolic syndrome. BMI is body mass index and is known to greatly increase diabetes risk as the increased weight increases insulin resistance. Stroke corresponds to whether and individual has ever had a stroke or not. HeartDiseaseorAttack corresponds to whether and individual has ever had a heart attack or has heart disease. The risk of both of these conditions is increased in diabetics as well. PhysActivity corresponds to whether an individual has exercised in the last 30 days. It does not indicate anything about frequency over those 30 days. HvyAlcoholConsump corresponds to whether and individual consumes a lot of alcohol. “Heavy” consumption is referred to as greater than 14 drinks a week for males and greater than 7 drinks a week for women. GenHlth is a variable includes 5 levels, used to categorized how an individual viewed their general health at the time of the survey. MentHlh and PhysHlth correspond to the number of days in the last 30 days the individual felt that their mental health or physical health were poor. DiffWalk is a yes/no response indicating whether the individual has difficulty walking or climbing stairs. Age indicates the age grouping of the individual. The 13 groupings start at age 18-24, with subsequent levels in increments of 5 years and the final level corresponding to 80+ years. Finally, income corresponds to 8 levels generally in 5K increments, ranging from less than 10K, to greater than 75k. The details of selecting these 13 predictor variables described here are discussed below for the EDA.\n\n\nEDA Analysis\nThe first step is to read in the data. Since the data is a csv file, this can be done using read_csv() from tidyverse.\n\n#load libraries\nlibrary(tidyverse)\nlibrary(psych)\n# read in data set\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\",\n                          show_col_types = FALSE)\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nThe first data step in EDA is to check for missing values. This data was already cleaned so missing values are not expected. Using is.na() shows that there are in fact no missing values.\n\n# check for missing values in each column\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThe distinct() function can be used to verify that there are no missing values in a form other than NA. Looking at the output below, we can see that there is nothing unusual. Most of the categories have 0’s and 1’s which generally correspond to no (0) and yes(1), with the exception of Fruits, Veggies and Sex. The categories with other numbers have various meanings depending on the category but the numbers are consistent with what is expected based on the kaggle website’s Diabetes Health Indicators Dataset Notebook.\n\n#check for unusual values that could indicate missingness\nunique_values &lt;- diabetes_data |&gt;\n  distinct()\nunique_values\n\n# A tibble: 229,474 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 229,464 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nThe next step is to summarize the variables. The simplest method for doing so is to use describe() from the psych package. Below is the summary for this data. As we would expect, most variables have a minimum of 0 and a maximum of 1. BMI, MentHlth and PhysHlth are the only variables that are truly numeric and this is reflected in the summary. One thing that stands out is that the max value for BMI is 98, while the average is 28.28. While a BMI of 98 is theoretically possible, it would likely be rare. This should be examined further.\n\n# summarize variables\ndescribe(diabetes_data)\n\n                     vars      n  mean   sd median trimmed  mad min max range\nDiabetes_binary         1 253680  0.14 0.35      0    0.05 0.00   0   1     1\nHighBP                  2 253680  0.43 0.49      0    0.41 0.00   0   1     1\nHighChol                3 253680  0.42 0.49      0    0.41 0.00   0   1     1\nCholCheck               4 253680  0.96 0.19      1    1.00 0.00   0   1     1\nBMI                     5 253680 28.38 6.61     27   27.68 4.45  12  98    86\nSmoker                  6 253680  0.44 0.50      0    0.43 0.00   0   1     1\nStroke                  7 253680  0.04 0.20      0    0.00 0.00   0   1     1\nHeartDiseaseorAttack    8 253680  0.09 0.29      0    0.00 0.00   0   1     1\nPhysActivity            9 253680  0.76 0.43      1    0.82 0.00   0   1     1\nFruits                 10 253680  0.63 0.48      1    0.67 0.00   0   1     1\nVeggies                11 253680  0.81 0.39      1    0.89 0.00   0   1     1\nHvyAlcoholConsump      12 253680  0.06 0.23      0    0.00 0.00   0   1     1\nAnyHealthcare          13 253680  0.95 0.22      1    1.00 0.00   0   1     1\nNoDocbcCost            14 253680  0.08 0.28      0    0.00 0.00   0   1     1\nGenHlth                15 253680  2.51 1.07      2    2.45 1.48   1   5     4\nMentHlth               16 253680  3.18 7.41      0    1.04 0.00   0  30    30\nPhysHlth               17 253680  4.24 8.72      0    1.77 0.00   0  30    30\nDiffWalk               18 253680  0.17 0.37      0    0.09 0.00   0   1     1\nSex                    19 253680  0.44 0.50      0    0.43 0.00   0   1     1\nAge                    20 253680  8.03 3.05      8    8.17 2.97   1  13    12\nEducation              21 253680  5.05 0.99      5    5.15 1.48   1   6     5\nIncome                 22 253680  6.05 2.07      7    6.35 1.48   1   8     7\n                      skew kurtosis   se\nDiabetes_binary       2.08     2.34 0.00\nHighBP                0.29    -1.92 0.00\nHighChol              0.31    -1.91 0.00\nCholCheck            -4.88    21.83 0.00\nBMI                   2.12    11.00 0.01\nSmoker                0.23    -1.95 0.00\nStroke                4.66    19.69 0.00\nHeartDiseaseorAttack  2.78     5.72 0.00\nPhysActivity         -1.20    -0.57 0.00\nFruits               -0.56    -1.69 0.00\nVeggies              -1.59     0.54 0.00\nHvyAlcoholConsump     3.85    12.85 0.00\nAnyHealthcare        -4.18    15.48 0.00\nNoDocbcCost           3.00     6.97 0.00\nGenHlth               0.42    -0.38 0.00\nMentHlth              2.72     6.44 0.01\nPhysHlth              2.21     3.50 0.02\nDiffWalk              1.77     1.15 0.00\nSex                   0.24    -1.94 0.00\nAge                  -0.36    -0.58 0.01\nEducation            -0.78     0.04 0.00\nIncome               -0.89    -0.28 0.00\n\n\nTo examine the BMI category, we can look at the unique values in the category. By doing so we can see the range of values present.\n\nunique_values &lt;- unique(diabetes_data$BMI)\nunique_values\n\n [1] 40 25 28 27 24 30 34 26 33 21 23 22 38 32 37 31 29 20 35 45 39 19 47 18 36\n[26] 43 55 49 42 17 16 41 44 50 59 48 52 46 54 57 53 14 15 51 58 63 61 56 74 62\n[51] 64 66 73 85 60 67 65 70 82 79 92 68 72 88 96 13 81 71 75 12 77 69 76 87 89\n[76] 84 95 98 91 86 83 80 90 78\n\n\nThe distribution of these values can be visualized using a box plot.\n\n#plot distribution of BMI between 50 and 100\nggplot(diabetes_data, aes(x = BMI)) +\n  geom_boxplot(fill = \"darksalmon\")\n\n\n\n\n\n\n\n\nTo see if these values might potentially be outliers, z-scores can be calculated. Generally, anything outside of +/- 3 is usually considered an outlier. This identifies 2963 potential outliers, in a BMI range of 49-98. This accounts for 1.17% of the data.\n\n#compute z-scores\ndiabetes_z&lt;- diabetes_data |&gt;\n  mutate(z_score = (BMI - 28.38) / 6.61)\n#view and determine number of outliers\ndiabetes_z_outlier &lt;- diabetes_z |&gt;\n  filter(abs(z_score) &gt; 3)\ndiabetes_z_outlier\n\n# A tibble: 2,963 × 23\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               1      1        1         1    55      0      0\n 2               0      1        1         1    49      1      1\n 3               1      1        1         1    49      0      1\n 4               0      0        0         1    50      1      0\n 5               1      1        0         1    59      0      0\n 6               0      1        0         1    50      0      0\n 7               1      1        1         1    52      1      0\n 8               0      0        0         1    55      0      0\n 9               0      1        1         1    54      1      0\n10               0      0        0         0    49      1      0\n# ℹ 2,953 more rows\n# ℹ 16 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;,\n#   z_score &lt;dbl&gt;\n\n\nMorbid obesity is considered to be anything over 40. According to the National Health and Nutrition Examination Survey (NHANES), 9.2% of adults were considered morbidly obese in 2018. To see how this aligns with the diabetes indicators set , counts can be calculated. By the counts in this data, 4.5% of individuals are morbidly obese. Since this is around half of the actual population incidence, the values classified as potential outliers by z-score will be retained to keep the data set as close to a population representation as possible.\n\ndiabetes_data |&gt;\n  filter(BMI &gt;40) |&gt;\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 11479\n\n\nFor the most of the data, since it is categorical, we can look at contingency tables to get an idea of their relationship with diabetes status. For this purpose, the data is going to be manipulated first. The categorical variables will be converted to factors and the levels renamed to better communicate what the levels actually mean. The factor() function will be used as it allows renaming of levels and specifying if a factor is ordered, as some of the categories are.\n\n#convert categorical columns to factors\n#Remove BMI outliers\ndiabetes &lt;- diabetes_data |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1),\n                                  labels = \n                                    c(\"Non_Diabetic\", \"Prediabetic_Diabetic\")),\n         Education = factor(Education, ordered = TRUE, levels = c(1:6),\n                            labels = c(\"No_School\", \"Primary_and_Middle\",\n                                       \"Some_High_School\", \n                                       \"Graduated_High_School\", \"Some_College\",\n                                       \"Graduated_College\")),\n         Age = factor(Age,ordered = TRUE, levels = c(1:13), \n                       labels = c(\"Age_18to24\", \"Age_25to29\", \n                                  \"Age_30to34\", \"Age_35to39\",\n                                  \"Age_40to44\", \"Age_45to49\", \n                                  \"Age_50to54\", \"Age_55to59\", \n                                  \"Age_60to64\", \"Age_65to69\", \n                                  \"Age_70to74\", \"Age_75to79\", \n                                  \"Age_80_or_above\")),\n         Income = factor(Income, ordered = TRUE, levels = c(1:8),\n                         labels = c(\"Less_than_10K\", \n                                    \"From_10K_to_under_15K\",\n                                    \"From_15K_to_under_20K\",\n                                    \"From_20K_to_under_25K\", \n                                    \"From_25K_to_under_35K\",\n                                    \"From_35K_to_under_50K\", \n                                    \"From_50K_to_under_75K\",\n                                    \"From_75K_or_more\")),\n         Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n         DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = \n                                c(\"No\", \"Yes\")),\n         AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), \n                                labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), \n                                    labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity, levels = c(0, 1), \n                               labels = c(\"No\", \"Yes\")),\n         HeartDiseaseorAttack = factor(HeartDiseaseorAttack, \n                                       levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits, levels = c(0, 1), labels = \n                           c(\"None\", \"One_or_more_per_day\")),\n         Veggies = factor(Veggies, levels = c(0, 1), \n                          labels = c(\"None\", \"One_or_more_per_day\")),\n         GenHlth = factor(GenHlth, levels = c(1:5), \n                          labels = c(\"Excellent\", \"Very_Good\", \"Good\", \n                                     \"Fair\", \"Poor\")))\ndiabetes\n\n# A tibble: 253,680 × 22\n   Diabetes_binary      HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;                &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 Non_Diabetic         Yes    Yes      Yes          40 Yes    No    \n 2 Non_Diabetic         No     No       No           25 Yes    No    \n 3 Non_Diabetic         Yes    Yes      Yes          28 No     No    \n 4 Non_Diabetic         Yes    No       Yes          27 No     No    \n 5 Non_Diabetic         Yes    Yes      Yes          24 No     No    \n 6 Non_Diabetic         Yes    Yes      Yes          25 Yes    No    \n 7 Non_Diabetic         Yes    No       Yes          30 Yes    No    \n 8 Non_Diabetic         Yes    Yes      Yes          25 Yes    No    \n 9 Prediabetic_Diabetic Yes    Yes      Yes          30 Yes    No    \n10 Non_Diabetic         No     No       Yes          24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;ord&gt;, Education &lt;ord&gt;, Income &lt;ord&gt;\n\n\nNow that the data is properly formatted, contingency tables can be generated. The first contingency table for the Sex variable is shown below. The distribution of males and females appears to be relatively similar, with males appearing to have a slightly higher propensity to have prediabetes or diabetes compared to women. Because the difference amounts to less than a few percent, this variable will be excluded from the modeling process. Note that this is just a visual observation and conclusion to eliminate variables for different models to be used alongside the full data set in the model fitting process. Normally, statistical significance would be determined, which can be a more difficult process with the imbalance in the data.\n\n#Contingency table for sex\ndiabetes |&gt;\n  group_by(Sex, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Sex    Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 Female       123563                18411\n2 Male          94771                16935\n\n\nWe can also use bar charts to visualize the proportions of category levels relative to the diabetes variable. This visualization makes it easier to see the similarities or differences in proportions.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = Sex)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Sex of Individuals by Diabetes Status\") +\n  scale_fill_discrete(name=\"Sex Drinker\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nIn looking at income, the prevalence of diabetes generally decreases as income increases, indicating that it may be an important predictor of diabetes. This variable will be included in the model fitting process.\n\n#Contingency table for income\ndiabetes |&gt;\n  group_by(Income, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 8 × 3\n  Income                Non_Diabetic Prediabetic_Diabetic\n  &lt;ord&gt;                        &lt;int&gt;                &lt;int&gt;\n1 Less_than_10K                 7428                 2383\n2 From_10K_to_under_15K         8697                 3086\n3 From_15K_to_under_20K        12426                 3568\n4 From_20K_to_under_25K        16081                 4054\n5 From_25K_to_under_35K        21379                 4504\n6 From_35K_to_under_50K        31179                 5291\n7 From_50K_to_under_75K        37954                 5265\n8 From_75K_or_more             83190                 7195\n\n\nThe bar chart for better visualization of the Income variable is shown below.\n\nggplot(diabetes, aes(x = Income, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without Diabetes by Income Level\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\")) +\n  theme(axis.text.x = element_text(size = 9, angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext, we will examine the contingency table for education. Based on the output, it appears that diabetes decreases with income. This is similar to the contingency table for income, which makes intuitive sense since income is generally associated with level of education.\n\n#Contingency table for Education\ndiabetes |&gt;\n  group_by(Education, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 6 × 3\n  Education             Non_Diabetic Prediabetic_Diabetic\n  &lt;ord&gt;                        &lt;int&gt;                &lt;int&gt;\n1 No_School                      127                   47\n2 Primary_and_Middle            2860                 1183\n3 Some_High_School              7182                 2296\n4 Graduated_High_School        51684                11066\n5 Some_College                 59556                10354\n6 Graduated_College            96925                10400\n\n\nLooking at the bar chart, the patterns for income and education look nearly identical.\n\nggplot(diabetes, aes(x = Education, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without Diabetes by Education Level\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThe potential relationship between education and income should be examined because these two variables could be correlated and therefore redundant, which could affect the model predictions. To get a view of the relationship, a contingency table can be created. In looking at the output below, it appears that the number of individuals generally decreases as income level increases at the lower education levels, and increases at the higher education levels, suggesting that these variables are correlated.\n\n#contingency table for education and income\ncont_table &lt;- diabetes |&gt;\n  group_by(Education, Income) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Income, values_from = count)\ncont_table\n\n# A tibble: 6 × 9\n  Education            Less_than_10K From_10K_to_under_15K From_15K_to_under_20K\n  &lt;ord&gt;                        &lt;int&gt;                 &lt;int&gt;                 &lt;int&gt;\n1 No_School                       37                    25                    28\n2 Primary_and_Middle             900                   741                   740\n3 Some_High_School              1536                  1465                  1709\n4 Graduated_High_Scho…          3594                  4692                  6511\n5 Some_College                  2437                  3315                  4664\n6 Graduated_College             1307                  1545                  2342\n# ℹ 5 more variables: From_20K_to_under_25K &lt;int&gt;, From_25K_to_under_35K &lt;int&gt;,\n#   From_35K_to_under_50K &lt;int&gt;, From_50K_to_under_75K &lt;int&gt;,\n#   From_75K_or_more &lt;int&gt;\n\n\nWe can look into these variables further by testing independence with a Chi-Sqare test. The analysis, shown below, indicates that these variables are correlated, as indicated by the low p-value. Since education has a low frequency of numbers in the “No School” category, dropping this variable from the model fitting is a practical choice.\n\n#make a matrix and remove the Education column names\ncont_matrix &lt;- as.matrix(cont_table[, -1])\n#Chi-Square test\nchisquare_test &lt;- chisq.test(cont_matrix)\nchisquare_test\n\n\n    Pearson's Chi-squared test\n\ndata:  cont_matrix\nX-squared = 60337, df = 35, p-value &lt; 2.2e-16\n\n\nNext, we will examine the contingency table for age. Based on the output, it appears that the incidence of diabetes generally increases with age, so this variable will be included in the model fitting.\n\n#Contingency table for age\ndiabetes |&gt;\n  group_by(Age, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 13 × 3\n   Age             Non_Diabetic Prediabetic_Diabetic\n   &lt;ord&gt;                  &lt;int&gt;                &lt;int&gt;\n 1 Age_18to24              5622                   78\n 2 Age_25to29              7458                  140\n 3 Age_30to34             10809                  314\n 4 Age_35to39             13197                  626\n 5 Age_40to44             15106                 1051\n 6 Age_45to49             18077                 1742\n 7 Age_50to54             23226                 3088\n 8 Age_55to59             26569                 4263\n 9 Age_60to64             27511                 5733\n10 Age_65to69             25636                 6558\n11 Age_70to74             18392                 5141\n12 Age_75to79             12577                 3403\n13 Age_80_or_above        14154                 3209\n\n\nA visualization of the Age variable is presented below.\n\nggplot(diabetes, aes(x = Age, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of Individuals with and without Diabetes by Age\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext, the relationship between the diabetes variable and Fruits will be examined. The proportions for no fruit eaten for diabetics and non-diabetics is fairly similar (0.36 vs 0.41) so this variable will be excluded from modeling.\n\n#Contingency table for Fruits\ndiabetes |&gt;\n  group_by(Fruits, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Fruits              Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                      &lt;int&gt;                &lt;int&gt;\n1 None                       78129                14653\n2 One_or_more_per_day       140205                20693\n\n\nSimilarly, vegetable consumption does not appear to be an important variable for diabetes classification, with proportions for no fruit eaten for diabetics and non-diabetics being 0.18 and 0.24 respectively.\n\n#Contingency table for Veggies\ndiabetes |&gt;\n  group_by(Veggies, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Veggies             Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                      &lt;int&gt;                &lt;int&gt;\n1 None                       39229                 8610\n2 One_or_more_per_day       179105                26736\n\n\nThe contingency table for PhysActivity shows that the proportion of diabetics who exercised is 0.777 compared to 0.631 for non-diabetics. This variable may be important for diabetes prediction, although it’s relevance may be more closely tied to how much a person exercises in the time period specified. The question to respondents was if they had engaged in any physical activity in the last 30 days so it does not take into account frequency of exercise in that time period. However, many people who exercise at all try to do so regularly, so this variable will be included in the modeling process.\n\n#Contingency table for PhysActivity\ndiabetes |&gt;\n  group_by(PhysActivity, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  PhysActivity Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;               &lt;int&gt;                &lt;int&gt;\n1 No                  48701                13059\n2 Yes                169633                22287\n\n\nThe bar chart for PhysActivity is presented below.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = PhysActivity)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Physical Activity\") +\n  scale_fill_discrete(name=\"Physical Activity\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nThe HvyAlcoholConsump variable may be important for diabetes prediction. The contingency table below shows that the proportion of non-diabetic individuals who drink alcohol heavily is close to 3 times that of diabetics.\n\n#Contingency table for HvyAlcoholConsum\ndiabetes |&gt;\n  group_by(HvyAlcoholConsump, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HvyAlcoholConsump Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                    &lt;int&gt;                &lt;int&gt;\n1 No                      204910                34514\n2 Yes                      13424                  832\n\n\nA bar chart for heavy alcohol consumption is below.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = HvyAlcoholConsump)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Heavy Drinking\") +\n  scale_fill_discrete(name=\"Heavy Drinker\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nThe NoDocbcCost represents whether an individual did or did not see a doctor because of the healthcare cost. The contingency table shows that the proportions of individuals with who did not see a doctor because of cost was similar for diabetics and non-diabetic, with proportions of 0.106 and 0.081, respectively. Because of the similarity, this variable will not be included in model fitting.\n\n#Contingency table for NoDocbcCost\ndiabetes |&gt;\n  group_by(NoDocbcCost, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  NoDocbcCost Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;              &lt;int&gt;                &lt;int&gt;\n1 No                200722                31604\n2 Yes                17612                 3742\n\n\nThe AnyHealthcare variable is similar in proportion for those who did not have healthcare, with proportions of 0.04 and 0.05 for diabetics and non-diabetics respectively. This variable will be excluded from model fitting.\n\n#Contingency table for AnyHealthcare\ndiabetes |&gt;\n  group_by(AnyHealthcare, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  AnyHealthcare Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                &lt;int&gt;                &lt;int&gt;\n1 No                   10995                 1422\n2 Yes                 207339                33924\n\n\nThe contingency table below for the HeartDiseaseorAttack variable suggests that this variable may be important for prediction. About 22% of diabetics had heart disease or a heart attack compared to 7% of those who did not have diabetes.\n\n#Contingency table for NHeartDiseaseorAttack\ndiabetes |&gt;\n  group_by(HeartDiseaseorAttack, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HeartDiseaseorAttack Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;                       &lt;int&gt;                &lt;int&gt;\n1 No                         202319                27468\n2 Yes                         16015                 7878\n\n\nBelow is a visualization of the HeartDiseaseorAttack variable.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = HeartDiseaseorAttack)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Heart Disease or Heart Attack\") +\n  scale_fill_discrete(name=\"Heart Disease or Attack\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nThe contingency table for stroke is shown below. Nearly 32% of those respondents who had a stroke had diabetes, compared to those who didn’t have a stroke (13%). This suggests that this variable may be important for prediction.\n\n#Contingency table for Stroke\ndiabetes |&gt;\n  group_by(Stroke, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Stroke Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 No           211310                32078\n2 Yes            7024                 3268\n\n\nThe Smoker variable does not appear to be particularly important for diabetes prediction and the proportions are similar. This seems contradictory since it is known that smokers are more likely to have diabetes since nicotine makes cells less responsive to insulin. But the question for respondents was “Have you smoked at least 100 cigarettes (5 packs) in your entire life?” which communicates nothing about when in their life that was and if they are currently smoking. Consequently, this variable will be left out of the model fitting.\n\n#Contingency table for Smoker\ndiabetes |&gt;\n  group_by(Smoker, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  Smoker Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 No           124228                17029\n2 Yes           94106                18317\n\n\nVisualization of smoking status is presented below in a bar chart.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = Smoker)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Smoking\") +\n  scale_fill_discrete(name=\"Smoker Status\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nThe contingency table for CholCheck shows that the proportions of those with and without diabetes who did not have their cholesterol checked in the last 5 years are 0.007 and 0.04, respectively. While the difference may be significant, people who are sick or have chronic conditions are more likely to have their cholesterol checked more frequently than healthy individuals. Thus this variable is not necessarily important for prediction, but rather reflective of diabetes management, which may also be accompanied by other health conditions. For these reasons, this variable will be excluded from the model fitting.\n\n#Contingency table for CholCheck\ndiabetes |&gt;\n  group_by(CholCheck, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  CholCheck Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;            &lt;int&gt;                &lt;int&gt;\n1 No                9229                  241\n2 Yes             209105                35105\n\n\nThe contingency table for DiffWalk is below. For individuals with diabetes, the proportion of those who responded yes to difficulty walking or climbing stairs is 0.371 compared 0.135 for those without diabetes. This variable may be important for predicting diabetes and will be included in model fitting.\n\n#Contingency table for DiffWalk\ndiabetes |&gt;\n  group_by(DiffWalk, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  DiffWalk Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;           &lt;int&gt;                &lt;int&gt;\n1 No             188780                22225\n2 Yes             29554                13121\n\n\nVisualization of DiffWalk relative to diabetes status is presented below.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = DiffWalk)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Ability to Walk/Climb Stairs\") +\n  scale_fill_discrete(name=\"Difficulty Walking\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nThe contingency table for HighChol shows proportions of 0.67 and 0.38 for high cholesterol for diabetics and non-diabetics, respectively. Since this is close to a 2-fold difference and diabetes and high blood pressure often occur together, this variable will be included in the model fitting.\n\n#Contingency table for HighChol\ndiabetes |&gt;\n  group_by(HighChol, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HighChol Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;           &lt;int&gt;                &lt;int&gt;\n1 No             134429                11660\n2 Yes             83905                23686\n\n\nA bar chart visualizing HighChol is below.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = HighChol)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Cholesterol\") +\n  scale_fill_discrete(name=\"High Cholesterol Status\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nSimilar to HighChol, HighBP was more common among diabetics than non-diabetics, with proportions off 0.75 and 0.38, respectively. High blood pressure often occurs in conjunction with diabetes as part of metabolic syndrome and so will be included in the model fitting.\n\n#Contingency table for HighBP\ndiabetes |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 2 × 3\n  HighBP Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;         &lt;int&gt;                &lt;int&gt;\n1 No           136109                 8742\n2 Yes           82225                26604\n\n\nVisualization for HighBP is below and similar to that of HighChol.\n\nggplot(diabetes, aes(x = Diabetes_binary, fill = HighBP)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", x = \"Diabetes Status\", title = \"Diabetes Status with Respect to Blood Pressure\") +\n  scale_fill_discrete(name=\"High Blood Pressure\") + \n  scale_x_discrete(labels = c(\"Non_Diabetic\" = \"Non-Diabetic\", \"Prediabetic_Diabetic\"= \"Prediabetic/Diabetic\"))\n\n\n\n\n\n\n\n\nWe can look at the distribution of BMI in this data by examining a histogram. The distribution is relatively normal with some right skew. As discussed previously, there are values in the 50-100 range that are not as apparent in this histogram. Since the data is unbalanced, with more respondents not having diabetes, the skewed portion of the histogram likely corresponds to individuals with diabetes. Generally speaking, a BMI between 18.5-24.9 corresponds to a normal weight.\n\nggplot(data = diabetes, aes(BMI)) +\n  geom_histogram(binwidth = 2, fill = \"darksalmon\", color = \"black\") +\n  labs(y = \"Count\")\n\n\n\n\n\n\n\n\nTo view the strength of the relationship between BMI and diabetes, we can use point-biserial correlation, which allows calculation of the correlation between a dichotomous variable and a continuous variable. Point-biserial correlation is a modified form of Pearson correlation and can be performed using cor.test(). Both variables need to be numeric. The output p-value and “true correlation” indicate that the correlation between diabetes and BMI is significant, which is consistent with what is known about type II diabetes. This variable will be included in modeling.\n\n#convert Diabetes_binary to numeric\ndiabetes_corr &lt;- diabetes |&gt;\n  select(Diabetes_binary, BMI) |&gt;\n  mutate(Diabetes_binary = as.numeric(Diabetes_binary))\n#Perform Point-Biserial Correlation\ncor.test(diabetes_corr$BMI, diabetes_corr$Diabetes_binary)\n\n\n    Pearson's product-moment correlation\n\ndata:  diabetes_corr$BMI and diabetes_corr$Diabetes_binary\nt = 111.88, df = 253678, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2131315 0.2205484\nsample estimates:\n      cor \n0.2168431 \n\n\nThe MentHlth variable measures the number of days out of the last 30 the individual felt that their physical health was not good. The means look different but the standard deviations are high.\n\ndiabetes |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(across(MentHlth, .fns = list(\"mean\" = mean,\n                                                 \"sd\" = sd),\n                                                 .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 3\n  Diabetes_binary      mean_MentHlth sd_MentHlth\n  &lt;fct&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n1 Non_Diabetic                  2.98        7.11\n2 Prediabetic_Diabetic          4.46        8.95\n\n\nThe significance of the difference in means can be determined using a t-test. The t-test indicates that the differences are significant, so this variable will be included in the modeling.\n\nt_test_MentHlth &lt;- t.test(MentHlth ~ Diabetes_binary, data = diabetes)\nt_test_MentHlth\n\n\n    Welch Two Sample t-test\n\ndata:  MentHlth by Diabetes_binary\nt = -29.695, df = 42872, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Non_Diabetic and group Prediabetic_Diabetic is not equal to 0\n95 percent confidence interval:\n -1.581710 -1.385835\nsample estimates:\n        mean in group Non_Diabetic mean in group Prediabetic_Diabetic \n                          2.978034                           4.461806 \n\n\nBelow is the distribution of the MentHlth variable. It is not normal, but given the number of observation, the t-test holds due to the Central Limit Theorem.\n\nggplot(data = diabetes, aes(MentHlth)) +\n  geom_histogram(binwidth = 2, fill = \"darksalmon\", color = \"black\") +\n  labs(y = \"Count\")\n\n\n\n\n\n\n\n\nThe PhysHlth variable measures the number of days out of the last 30 the individual felt that their physical health was not good. The means look different but the standard deviations are high.\n\ndiabetes |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(across(PhysHlth, .fns = list(\"mean\" = mean,\n                                                 \"sd\" = sd),\n                                                 .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 3\n  Diabetes_binary      mean_PhysHlth sd_PhysHlth\n  &lt;fct&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n1 Non_Diabetic                  3.64        8.06\n2 Prediabetic_Diabetic          7.95       11.3 \n\n\nThe significance of the difference in means for PhysHlth can be determined using a t-test. The t-test indicates that the differences are significant, so this variable will be included in the modeling.\n\nt_test_PhysHlth &lt;- t.test(PhysHlth ~ Diabetes_binary, data = diabetes)\nt_test_PhysHlth\n\n\n    Welch Two Sample t-test\n\ndata:  PhysHlth by Diabetes_binary\nt = -68.969, df = 41367, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Non_Diabetic and group Prediabetic_Diabetic is not equal to 0\n95 percent confidence interval:\n -4.435979 -4.190814\nsample estimates:\n        mean in group Non_Diabetic mean in group Prediabetic_Diabetic \n                          3.641082                           7.954479 \n\n\nBelow is the distribution of the PhysHlth variable. It is not normal, but given the number of observations, the t-test holds.\n\nggplot(data = diabetes, aes(PhysHlth)) +\n  geom_histogram(binwidth = 2, fill = \"darksalmon\", color = \"black\") +\n  labs(y = \"Count\")\n\n\n\n\n\n\n\n\nThe contingency table of the GenHlth variable shows that a significantly higher proportion of non-diabetics viewed their health as being “excellent” compared to diabetics (0.20 vs. 0.032). For each subsequent level of progressively worsening opinion of general health, the proportion of diabetics increased while that of non-diabetics decreased (0.034 vs. 0.130 for poor rating). This variable may be important for prediction and will be included in the model fitting.\n\n#Contingency table for GenHlth\ndiabetes |&gt;\n  group_by(GenHlth, Diabetes_binary) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n# A tibble: 5 × 3\n  GenHlth   Non_Diabetic Prediabetic_Diabetic\n  &lt;fct&gt;            &lt;int&gt;                &lt;int&gt;\n1 Excellent        44159                 1140\n2 Very_Good        82703                 6381\n3 Good             62189                13457\n4 Fair             21780                 9790\n5 Poor              7503                 4578\n\n\nA visualization of the general health variable is shown below.\n\nggplot(diabetes, aes(x = GenHlth, fill = Diabetes_binary)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"Count\", title = \"Proportion of individuals with and without \n       Diabetes by General Health Status\") +\n  scale_fill_discrete(name=\"Status\", labels = \n                        c(\"Not Diabetic\", \"Diabetic/Prediabetic\"))\n\n\n\n\n\n\n\n\n\nData export for Modeling\nA csv file is being created to import into the Modeling document. Unfortunately the categorical variables will need to be converted to factors again, but the labels will not need to be created again.\n\nwrite_csv(diabetes, \"diabetes.csv\")\n\n\n\n\nConclusion\nBased on the exploratory data analysis, the predictor variables have been reduced down to 13 from 21. This will be compared to two other models as discussed in the Modeling document. The model fitting can be accessed in the link below.\nModel Fitting"
  }
]